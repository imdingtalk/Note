<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>snoopy</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content><meta name=generator content="Hugo 0.83.1"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/dingtalk.pub/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css><link href=dingtalk.pub/posts/index.xml rel=alternate type=application/rss+xml title=snoopy><link href=dingtalk.pub/posts/index.xml rel=feed type=application/rss+xml title=snoopy><meta property="og:title" content="Posts"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="dingtalk.pub/posts/"><meta itemprop=name content="Posts"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content></head><body class="ma0 avenir bg-near-white"><header><div class="pb3-m pb6-l bg-black"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=dingtalk.pub/ class="f3 fw2 hover-white no-underline white-90 dib">snoopy</a><div class="flex-l items-center"></div></div></nav><div class="tc-l pv3 ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">Posts</h1></div></div></header><main class=pb7 role=main><article class="pa3 pa4-ns nested-copy-line-height nested-img"><section class="cf ph3 ph5-l pv3 pv4-l f4 tc-l center measure-wide lh-copy mid-gray"></section><section class="flex-ns flex-wrap justify-around mt5"><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1959886/ class="link black dim">mongodb使用pymongo批量插入数据</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">背景 在测试mongo相关功能的时候，需要批量插入数据，写一个脚本来批量插入 环境准备 安装 pymongo ，直接使用pip3安装，后面使用insert_many,insert_one的时候会报错如下，是因为默认安装的版本不是最新的，旧版本不支持最新的函数，直接安装最新的，pip3 install -U pymongo但是使用系统默认的python3，运行，会遇到各种问题，因为centos7系统默认的python3的path和pip3安装的东西不匹配，所以这里我直接用/usr/local/python3.6.4/bin,下的执行文件，而不是系统默认的/usr/bin/python3
TypeError: 'Collection' object is not callable. If you meant to call the 'insert_one' method on a 'Collection' object it is failing because no such method exists. 插入数据 import pymongo from pymongo import MongoClient #处理连接信息。建立连接 auth_str='cluster:cluster@' host_info='172.16.13.44:27017,172.16.13.44:27018,172.16.13.44:27019' MONGODB='test' param_str='?authMechanism=SCRAM-SHA-1&authSource=admin&replicaSet=rs0' uri='mongodb://%s%s/%s%s' % (auth_str, host_info, MONGODB, param_str) #print(uri) client = MongoClient(uri) #选项数据库插入数据 db=client.test db.test.count() list(db.test.find()) result = db.test.insert_many([{'x': i} for i in range(1000000)]) result.inserted_ids db.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1940892/ class="link black dim">处理api中关于operators.coreos.com的报错</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">背景  在测试环境中的监控一直报错，kubeapierrorhigh,经查看，是default，ns中存在异常的api，通过kubectl api-resources 发现该进程，已无法提供访问处理 删除对应的出问题的api
kubectl delete -n kube-system delete apiservice v1.packages.operators.coreos.com 删除对应的 api后，对应的告警没有了，至于具体这个出错的原因，始终没弄清楚，但是这个api不影响集群，所以先删除了</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1768801/ class="link black dim">mongodb副本集重新配置</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">背景 最开始部署mongodb副本集的时候使用的是本地地址，导致后面使用连接工具无法远程连接，故需要修改初始化的副本集配置 配置  > db.version(); 4.0.9 > conf=rs.conf() > conf.members[0].host="172.16.13.x:27017"; > conf.members[1].host="172.16.13.x:27018"; > conf.members[2].host="172.16.13.x:27019"; > rs.reconfig(conf,{"force":true});</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1499735/ class="link black dim">MYSQL高可用架构之MHA实践</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">简介 MHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案 经典架构 说明：1.MHA-manager可以安装在除MYSQL-Master以外的任意服务器，唯一的要求是能和所有的DB服务器进行SSH通信2.在所有的MYSQL服务器上安装MHA-node
工作流程 安装部署 keepalive配置 keep主要提供故障切换时候的VIP漂移 #安装 yum install keepalived -y 配置 global_defs { smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_MASTER } vrrp_instance VI_110 { interface ens33 virtual_router_id 110 nopreempt priority 110 advert_int 1 authentication { auth_type PASS auth_pass keepalived } virtual_ipaddress { 192.168.206.144 } } 根据 priority 字段自动判别主备，所以这个配置，在三台服务器之间应该不一致 数据库主从配置 配合5.6数据库新特性，配置主从变得异常简单，需要开启GTID，需要启用这三个参数，所有数据库添加该配置，先主库后从库，添加配置后，需要重启MySQL服务
vim /usr/local/mysql/my.cnf #GTID gtid_mode = on enforce_gtid_consistency = 1 log_slave_updates = 1 创建主从复制的用户，并在从库配置认主 #创建主从复制用户 grant replication slave on *.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1697464/ class="link black dim">kubeadm安装的集群的备份和恢复实践</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">说明 本文档简述了Kubernetes主节点灾备恢复的相关步骤，供在发生k8s master崩溃时操作 **环境：**kubeadm安装的k8s 1.14.1 实践 查看集群的成员和现存的key
ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ member list ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ get / --prefix --keys-only 备份的实现和恢复 etcd集群数据备份，这里我写了一个脚本实现
#!/bin/bash #脚本需要依赖etcdctl，安装 yum install etcd #获取脚本所存放目录 cd `dirname $0` bash_path=`pwd` #脚本名 me=$(basename $0) # delete dir and keep days delete_dirs=("/data/backup/kubernetes:7") backup_dir=/data/backup/kubernetes files_dir=("/etc/kubernetes" "/var/lib/kubelet") log_dir=$backup_dir/log shell_log=$log_dir/${USER}_${me}.log ssh_port="22" ssh_parameters="-o StrictHostKeyChecking=no -o ConnectTimeout=60" ssh_command="ssh ${ssh_parameters}-p ${ssh_port}" scp_command="scp ${ssh_parameters}-P ${ssh_port}" DATE=$(date +%F) TIME=$(date '+%Y%m%d%H') BACK_SERVER="127.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1676198/ class="link black dim">通过域控下发软件安装过程</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">背景 出于安全考虑，在域中的服务器需要批量安装某软件。 实施 脚本如下 do Set Of = CreateObject("Scripting.FileSystemObject") Set objShell = CreateObject("Shell.Application") dim OK,oShell OK=False set bag=getobject("winmgmts:\\.\root\cimv2") set pipe=bag.execquery("select * from win32_process where name='xxx.exe'") for each match in pipe OK = True msgbox "xxx.exe正在运行。。。" WScript.Quit Next If not OK Then msgbox "xxx.exe没有运行！" Set objShell = CreateObject("Wscript.Shell") WScript.Sleep 1000 msgbox "请手动完成xx客户端的安装，否则每次开机都提示",0 strCommandLine = "\\chinawyny.com\SysVol\domain.com\Policies\{5A850BAF-1901-4071-AD41-1AF69B9BEAA8}\User\Scripts\Logon\DSMClientSetup.exe" objShell.Run(strCommandLine) set WshShell = CreateObject("WScript.Shell") WScript.Sleep 1000 msgbox "要显示的内容",0 WScript.Quit end if loop 这里首先检测目标主机上是否安装了对应的程序并且已经自启动，如果已经启动，则提示软件已经在运行，如果没有运行则提示需要安装，如果是静默安装包，还可以实现静默安装，不弹出setup的安装步骤。软件的安装文件 路径为，域控服务器上的指定地址。 然后编辑域控的策略，添加该脚本即可 </div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1583929/ class="link black dim">K8S的污点（Taints ）和容忍（Tolerations）</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">节点亲和性是pod的一个属性，它将它们_吸引_到一组节点（作为首选项或硬性要求），Taints 是与之相反的，它允许节点驱逐或抵制一个pod 污点和容忍一起工作以确保不将pod安排到不适当的节点上。 将一个或多个污点应用于节点; 这标志着节点不应该接受任何不能容忍污点的pod。 容忍应用于容器，并允许（但不要求）容器安排到具有匹配的污点的节点上。 概念 使用kubectl taints为节点添加污点。例如 kubectl taint nodes node1 key=value:NoSchedule 在节点node1上放置污点。污点具有键key，值value和污点效果 NoSchedule。这意味着没有pod将能够调度到node1上，除非与pod具有匹配的容忍度。要删除上面命令添加的污点，您可以运行： kubectl taint nodes node1 key:NoSchedule- 在PodSpec中指定容器的容忍Tolerations。以下两种容忍度都“匹配”上面的kubectl污染线所产生的污点，因此具有任何容忍度的容器将能够安排到node1上：
tolerations: - key: "key" operator: "Equal" value: "value" effect: "NoSchedule" tolerations: - key: "key" operator: "Exists" effect: "NoSchedule" 如果key是相同的并且effect相同，则容忍和污点匹配，并且：  operator是Exists（在这种情况下不应指定key），或  operator是Equal，值相等  如果未指定，则operator默认为Equal。 **两种特殊情况： **
带有operator Exists的空键匹配所有key，value和effect，这意味着它将容忍所有内容。  tolerations: - operator: "Exists" 空effect使用key:key匹配所有effect。  tolerations: - key: "key" operator: "Exists" 上面的例子使用了NoSchedule的effect。或者，您可以使用PreferNoSchedule的效果。这是NoSchedule的“首选”或“软”版本 - 系统将尽量避免放置不能容忍节点上污点的pod，但这不是必需的。第三种效果是NoExecute，稍后描述。 您可以在同一节点上放置多个污点，并在同一个pod上放置多个容差。Kubernetes处理多个污点和容忍的方式就像一个过滤器：从所有节点的污点开始，然后忽略pod具有匹配容忍度的那些;剩下的未被忽视的污点对吊舱有明显的影响。特别是  如果至少有一个未被忽略的污点具有效果NoSchedule，那么Kubernetes将不会将pod安排到该节点上  如果没有未被忽略的污点，效果NoSchedule，但至少有一个未被忽略的污点有效PreferNoSchedule然后Kubernetes将尝试不将pod安排到节点  如果存在至少一个具有effect** ** **NoExecute**的未被忽略的污点，则该pod将从该节点逐出（如果它已经在该节点上运行），并且将不会被调度到该节点上（如果它尚未在该节点上运行）。  例如，假设您的节点有这样的污点： kubectl taint nodes node1 key1=value1:NoSchedule kubectl taint nodes node1 key1=value1:NoExecute kubectl taint nodes node1 key2=value2:NoSchedule pod有两种容忍度： tolerations: - key: "key1" operator: "Equal" value: "value1" effect: "NoSchedule" - key: "key1" operator: "Equal" value: "value1" effect: "NoExecute" 在这种情况下，pod将无法安排到节点上，因为没有容忍匹配第三种污点。但是，如果在添加污点时pod已经在节点上运行，它将能够继续运行，因为第三种污点是容器中不能容忍的三种污染中唯一的一种。（pod可以容忍node1有污点 key1=value1:NoSchedule和key1=value1:NoExecute，但是不容忍key2=value2:NoSchedule，但是key2的effect为NoSchedule，意味着在添加污点的时候如果pod已经在该node运行，则不采取处理。如果没有在该node运行，将不会调度到该节点。如果污点 key2=value2:NoExecute，pod容忍不变，那么即使已经在该node运行，添加了该污点后，pod也会被驱逐） 通常情况下，如果将一个带有NoExecute效果的污点添加到一个节点，那么任何不能容忍污染的pod都会立即被驱逐，任何容忍污染的pod都不会被驱逐。但是，对NoExecute效果的容忍可以指定一个可选的tolerationSeconds字段，该字段指示在添加污点后pod将保持绑定到节点的时间。例如， tolerations: - key: "key1" operator: "Equal" value: "value1" effect: "NoExecute" tolerationSeconds: 3600 意味着如果此pod正在运行并且将匹配的污点添加到节点，则pod将保持绑定到该节点3600秒，然后被逐出。如果在此之前删除了污点，则不会驱逐pod。 另外一个pod会被默认添加一个Tolerations： 以上的Tolerations意味着：如果node的状态处于not_ready状态超过5分钟，那么该pod将不能在node上继续执行，将被驱逐。 附注： kubernetes节点失效后pod的调度过程 1.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1585713/ class="link black dim">MySQL基于GTID复制的实践</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">介绍 GTID(Global Transaction ID)是MySQL5.6引入的功能，可以在集群全局范围标识事务，用于取代过去通过binlog文件偏移量定位复制位置的传统方式。借助GTID，在发生主备切换的情况下，MySQL的其它Slave可以自动在新主上找到正确的复制位置，这大大简化了复杂复制拓扑下集群的维护，也减少了人为设置复制位置发生误操作的风险。另外，基于GTID的复制可以忽略已经执行过的事务,减少了数据发生不一致的风险。 配置 enforce_gtid_consistency = ON gtid_mode = ON server_id = 9910 binlog_format = row 同时主备库都开启binlog如果不设置级联从库，从库不要设置log_slave_updates参数。这是最合理的设置。建立复制用户 #新版本中建立用户和授权需要分开执行 create user 'hh'@'%' identified by 'hh'; grant all on *.* to 'hh'@'%'; 导出主库数据
mysqldump -uroot -proot -h127.0.0.1 --single-transaction --master-data=2 -R -E --triggers --all-databases > test.sql 在从库source备份，然后使用MASTER_AUTO_POSITION建立同步 change master to master_host='xxx', master_user='repl', master_password='repl', master_port=3306, MASTER_AUTO_POSITION = 1; **启动slave ** start slave 如何修复复制错误 在基于GTID的复制拓扑中，要想修复Slave的SQL线程错误，过去的SQL_SLAVE_SKIP_COUNTER方式不再适用。需要通过设置gtid_next或gtid_purged完成，当然前提是已经确保主从数据一致，仅仅需要跳过复制错误让复制继续下去。比如下面的场景： 在从库上创建表tb1 mysql> set sql_log_bin=0; Query OK, 0 rows affected (0.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1539024/ class="link black dim">基于kubeadm搭建的etcd集群的运维实践</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">背景  kubeadm提供了两种高可用方案，用户搭建k8s的高可用环境，详细对比可参考： Options for Highly Available Topology 堆叠的etcd拓扑 特点：
etcd节点和apiserver以及scheduler，controller-manager共存，需要较小的服务器资源  如果node出现问题，那么master节点和etcd节点都将下线 这是kubeadm中的默认拓扑。使用kubeadm init和kubeadm join时使用 experimental-control-plane 作为master 加入集群时候，在控制平面节点上自动创建本地etcd成员 外部etcd集群 特点：
需要额外的服务器搭建etcd集群，相对于堆叠型拓扑，需要双倍的服务器节点 etcd节点和master分离，不像堆叠型拓扑，master或者etcd节点宕机影响更小 常用命令 #etcd需要在etcd的pod内执行 kubectl exec -it -n kube-system etcd-kube-node1 sh #查看集群成员 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ member list #健康检查 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ --endpoints=https://ip1:2379,https://ip2:2379,\ https://ip3:2379 \ endpoint health #删除成员 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ member remove [member ID] #添加一个值 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=dingtalk.pub/posts/1530580/ class="link black dim">快速批量删除pod</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">背景  因为宿主机资源关系，产生了较多被驱逐的pod，如果不清除，将会一直显示，即使pod已经被重新调度 如何删除 #删除默认ns下的被驱逐的pod kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod #删除指定空间下被驱逐的pod kubectl get pods -n kube-system | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n kube-system</div></div></div></div></section><ul class=pagination><li class=page-item><a href=dingtalk.pub/posts/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=dingtalk.pub/posts/page/5/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=dingtalk.pub/posts/>1</a></li><li class=page-item><a class=page-link href=dingtalk.pub/posts/page/2/>2</a></li><li class=page-item><a class=page-link href=dingtalk.pub/posts/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=dingtalk.pub/posts/page/5/>5</a></li><li class="page-item active"><a class=page-link href=dingtalk.pub/posts/page/6/>6</a></li><li class=page-item><a class=page-link href=dingtalk.pub/posts/page/7/>7</a></li><li class=page-item><a href=dingtalk.pub/posts/page/7/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=dingtalk.pub/posts/page/7/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=dingtalk.pub>&copy; snoopy 2021</a><div></div></div></footer></body></html>