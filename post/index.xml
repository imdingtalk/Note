<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on snoopy</title><link>dingtalk.pub/post/</link><description>Recent content in Posts on snoopy</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 09 Jun 2021 06:45:23 +0000</lastBuildDate><atom:link href="dingtalk.pub/post/index.xml" rel="self" type="application/rss+xml"/><item><title>ubuntu 配置 clash</title><link>dingtalk.pub/post/45240324/</link><pubDate>Wed, 09 Jun 2021 06:45:23 +0000</pubDate><guid>dingtalk.pub/post/45240324/</guid><description>安装 下载最新版本 clash：https://github.com/Dreamacro/clash/releases​
自己的阿里云网盘存档一份： https://www.aliyundrive.com/​ 初始化执行clash ./clash 配置文件参考 https://github.com/Dreamacro/clash/wiki/configuration注意配置控制台端口号Web 配置端口号9090可以从配置文件的external-controller找到
port: 7890 socks-port: 7891 allow-lan: true mode: Rule log-level: info external-controller: :9090 proxies: .... .... 使用 Web 工具管理 clash web端管理地址： http://clash.razord.top/#/proxies​配置开机自启动 sudo vim /usr/lib/systemd/system/clash.service [Unit] Description=clash After=network.target [Service] ExecStart=/usr/bin/nohup /home/biao/soft/clash -f /home/biao/soft/ss.yaml &amp;amp; User=biao [Install] WantedBy=multi-user.target 自启动 sudo systemctl start clash sudo systemctl enable clash</description></item><item><title>nerdctl 小探</title><link>dingtalk.pub/post/43680688/</link><pubDate>Fri, 16 Apr 2021 03:30:33 +0000</pubDate><guid>dingtalk.pub/post/43680688/</guid><description>介绍 Nerdctl是一个用于Containerd的Docker兼容的CLI 安装和使用 https://github.com/containerd/nerdctl/releases 特性 lazy-pulling  安装 Stargz 插件 (containerd-stargz-grpc) &amp;gt;https://github.com/containerd/stargz-snapshotter #/etc/systemd/system/stargz.service [Unit] Description=BuildKit Documentation=https://github.com/moby/buildkit [Service] ExecStart=/usr/local/bin/containerd-stargz-grpc [Install] WantedBy=multi-user.target systemctl enable stargz 给conrainerd添加如下配置 /etc/containerd/config.toml
[proxy_plugins] [proxy_plugins.stargz] type = &amp;#34;snapshot&amp;#34; address = &amp;#34;/run/containerd-stargz-grpc/containerd-stargz-grpc.sock&amp;#34; #配合k8s使用 &amp;gt;https://github.com/containerd/stargz-snapshotter#quick-start-with-kubernetes# 测试
# 重启一下 containerd 和 containerd-stargz-grpc systemctl restart containerd.service systemctl restart stargz.service # 尝试拉取一个镜像 #使用stargz root@biao:/tmp# nerdctl --snapshotter=stargz pull ghcr.io/stargz-containers/fedora:30-esgz ghcr.io/stargz-containers/fedora:30-esgz: resolved |++++++++++++++++++++++++++++++++++++++| index-sha256:a4d6c9d2a329aa83d4ffbc1e2bb1fb423c6920ee930afb4aba01b2fecb575b53: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:36a9dc91f23e6baa88a59b7d657b0a16640d38c6cecaee659bd9dfeeec98ac5f: done |++++++++++++++++++++++++++++++++++++++| config-sha256:f42ed93c71d834aad9dc6892078a452d1c3e4968db206b74f111123bb6efd65e: done |++++++++++++++++++++++++++++++++++++++| elapsed: 5.</description></item><item><title>[go笔记] new 和 make</title><link>dingtalk.pub/post/33602495/</link><pubDate>Thu, 25 Mar 2021 15:59:14 +0000</pubDate><guid>dingtalk.pub/post/33602495/</guid><description>new 返回类型的指针，指向类型的指针地址，置为该类型的0值make 返回类型本身，用于map,slice,chan值类型会直接分配内存，引用类型需要申请内存Each element of such a variable or value is set to the zero value for its type: false for booleans, 0 for numeric types, &amp;quot;&amp;quot; for strings, and nil for pointers, functions, interfaces, slices, channels, and maps
在使用make()函数创建切片时，如果我们能够预计出合理的容量大小（太大浪费内存空间，太小会不断的扩容），哪么我们在进行切片的append时，可能不会发生扩容，也就避免了切片元素的复制，减少了开销。
n2 := new([]int) fmt.Println(&amp;#34;result&amp;#34;, &amp;amp;n2,n2, *n2) //new 切片,0值为[] fmt.Println(len(*n2), cap(*n2)) n3:=make([]int,3,6) fmt.Println(&amp;#34;result&amp;#34;,n3)</description></item><item><title>kubeadm集群证书管理相关</title><link>dingtalk.pub/post/6554913/</link><pubDate>Tue, 23 Mar 2021 08:26:16 +0000</pubDate><guid>dingtalk.pub/post/6554913/</guid><description>检查证书过期时间 Kubelet使用证书对Kubernetes API进行身份验证。默认情况下，这些证书的有效期为一年，因此不需要太频繁地更新它们。
#对应1.15以上的kubeadm集群 kubeadm alpha certs check-expiration #其他集群 openssl x509 -noout -dates -in /etc/kubernetes/pki/*.crt 证书到期的影响 1. kubectl 不可用 [root@biao ~]# date -s &amp;#34;2022-06-25 20:13:00&amp;#34; Sat Jun 25 20:13:00 CST 2022 [root@biao ~]# kubectl get pod Unable to connect to the server: x509: certificate has expired or is not yet valid 2. 系统相关的组件，etcd,kube-controller-manager,kube-scheduler通通不可用，集群全面崩溃 # 查看日志信息，是在证书更新后使用新的kubeconfig文件来查看的 [root@biao kubernetes]# kubectl logs -n kube-system etcd-biao Error from server (InternalError): Internal error occurred: Authorization error (user=kube-apiserver-kubelet-client, verb=get, resource=nodes, subresource=proxy) [root@biao kubernetes]# kubectl logs -n kube-system kube-controller-manager-biao Error from server (InternalError): Internal error occurred: Authorization error (user=kube-apiserver-kubelet-client, verb=get, resource=nodes, subresource=proxy [root@biao kubernetes]# kubectl logs -n kube-system kube-scheduler-biao Error from server (InternalError): Internal error occurred: Authorization error (user=kube-apiserver-kubelet-client, verb=get, resource=nodes, subresource=proxy) [root@biao kubernetes]# kubectl logs -n kube-system kube-apiserver-biao Error from server (InternalError): Internal error occurred: Authorization error (user=kube-apiserver-kubelet-client, verb=get, resource=nodes, subresource=proxy) 证书更新以及相关更新 #更新所有的证书 #准备集群配置文件 kubeadm config view &amp;gt; /tmp/cluster.</description></item><item><title>配置对Harbor的HTTPS访问</title><link>dingtalk.pub/post/29816377/</link><pubDate>Mon, 15 Mar 2021 07:15:56 +0000</pubDate><guid>dingtalk.pub/post/29816377/</guid><description>生成证书颁发机构证书 在生产环境中，应该从CA获得证书。在测试或开发环境中，可以生成自己的CA。要生成CA证书，运行以下命令。
生成CA证书私钥。 openssl genrsa -out ca.key 4096 生成CA证书。调整-subj选项中的值以反映您的组织。如果使用FQDN连接Harbor主机，则必须将其指定为通用名称（CN）属性。 CN=yourdomain.com openssl req -x509 -new -nodes -sha512 -days 3650 \ -subj &amp;quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=$CN&amp;quot; \ -key ca.key \ -out ca.crt 生成服务器证书 证书通常包含一个.crt文件和一个.key文件，例如yourdomain.com.crt和yourdomain.com.key。
生成私钥。 openssl genrsa -out $CN.key 4096 生成证书签名请求（CSR）。调整-subj选项中的值以反映您的组织。如果使用FQDN连接Harbor主机，则必须将其指定为公用名（CN）属性，并在密钥和CSR文件名中使用它。 openssl req -sha512 -new \ -subj &amp;quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=$CN&amp;quot; \ -key $CN.key \ -out $CN.csr 生成一个x509 v3扩展文件。无论您使用FQDN还是IP地址连接到Harbor主机，都必须创建此文件，以便可以为您的Harbor主机生成符合主题备用名称（SAN）和x509 v3的证书扩展要求。替换DNS条目以反映您的域。 cat &amp;gt; v3.ext &amp;lt;&amp;lt;-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.</description></item><item><title>小试linkerd</title><link>dingtalk.pub/post/32630466/</link><pubDate>Wed, 10 Mar 2021 07:32:28 +0000</pubDate><guid>dingtalk.pub/post/32630466/</guid><description>介绍 Linkerd是Kubernetes的_服务网格_。通过提供运行时调试，可观察性，可靠性和安全性，它使运行服务变得更加轻松和安全，而所有这些都无需更改代码。 安装 安装cli To install the CLI manually, run:
curl -sL https://run.linkerd.io/install | sh  add linkerd to your path with:
export PATH=$PATH:$HOME/.linkerd2/bin 验证Kubernetes集群 linkerd check --pre 将Linkerd安装到集群上 linkerd install | kubectl apply -f - #通过运行以下命令来验证安装： linkerd check 访问dashboard linkerd dashboard &amp;amp; 应用 怎么给应用注入
kubectl get -n emojivoto deploy -o yaml \ | linkerd inject - \ | kubectl apply -f - 观察应用
linkerd -n emojivoto stat deploy 它给应用带来了什么 ，默认的mTLS linkerd -n linkerd edges deployment linkerd -n linkerd tap deploy</description></item><item><title>cert-manager 的安装和使用</title><link>dingtalk.pub/post/32642359/</link><pubDate>Wed, 10 Mar 2021 07:27:56 +0000</pubDate><guid>dingtalk.pub/post/32642359/</guid><description>介绍 cert-manager是本地Kubernetes证书管理控制器。它可以帮助从各种来源颁发证书，例如Let&amp;rsquo;s Encrypt， HashiCorp Vault， Venafi，简单的签名密钥对或自签名。它将确保证书有效并且是最新的，并在到期前尝试在配置的时间续订证书。 安装 首次安装 $ helm repo add jetstack https://charts.jetstack.io $ helm repo update $ helm install \ cert-manager jetstack/cert-manager \ --namespace cert-manager \ --version v1.2.0 \ --create-namespace \ # --set installCRDs=true 升级 $ helm upgrade --version &amp;lt;version&amp;gt; &amp;lt;release_name&amp;gt; jetstack/cert-manager 使用</description></item><item><title>helm 中如何拼接一个变量</title><link>dingtalk.pub/post/32005266/</link><pubDate>Fri, 26 Feb 2021 07:46:53 +0000</pubDate><guid>dingtalk.pub/post/32005266/</guid><description>使用 定义一个变量
{{ define &amp;#34;makeServiceNamespace&amp;#34; }} {{- if .Values.serviceTag }} {{- printf &amp;#34;%s-%s&amp;#34; .Values.serviceNamespace .Values.serviceTag -}} {{- else }} {{- print .Values.serviceNamespace }} {{- end }} {{- end }} 使用该变量
serviceNamespace: {{ template makeServiceNamespace . }} 参考 https://stackoverflow.com/questions/45278655/kubernetes-helm-combine-two-variables-with-a-string-in-the-middle</description></item><item><title>falco</title><link>dingtalk.pub/post/29696468/</link><pubDate>Wed, 06 Jan 2021 07:49:18 +0000</pubDate><guid>dingtalk.pub/post/29696468/</guid><description>falco是什么？ falco项目是最初由Sysdig，Inc构建的开源运行时安全工具。Falco被捐赠给CNCF，现在是CNCF孵化项目。 falco做什么？ Falco通过以下方式使用系统调用来保护和监视系统：
在运行时从内核解析Linux系统调用 针对强大的规则引擎声明流 违反规则时发出警报 安装 curl -s https://falco.org/repo/falcosecurity-3672BA8F.asc | apt-key add - echo &amp;#34;deb https://dl.bintray.com/falcosecurity/deb stable main&amp;#34; | tee -a /etc/apt/sources.list.d/falcosecurity.list apt-get update -y apt-get -y install linux-headers-$(uname -r) apt-get install -y falco 配置</description></item><item><title>k8s中的强制删除-pod or namespace</title><link>dingtalk.pub/post/3593792/</link><pubDate>Fri, 04 Dec 2020 06:14:21 +0000</pubDate><guid>dingtalk.pub/post/3593792/</guid><description>1. 强制删除 kubectl delete namespace &amp;lt;NAMESPACE&amp;gt; --force --grace-period=0 2. 使用API操作 kubectl proxy &amp;amp; NAMESPACE=monitoring kubectl get namespace $NAMESPACE -o json |jq '.spec = {&amp;quot;finalizers&amp;quot;:[]}' &amp;gt;temp.json curl -k -H &amp;quot;Content-Type: application/json&amp;quot; -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize 3. 在etcd中删除 查看
ETCDCTL_API=3 etcdctl get /registry/namespaces --prefix --keys-only \ --cert /etc/kubernetes/pki/etcd/peer.crt \ --key /etc/kubernetes/pki/etcd/peer.key \ --cacert /etc/kubernetes/pki/etcd/ca.crt 删除
ETCDCTL_API=3 etcdctl del /registry/namespaces/&amp;lt;NAMESPACE&amp;gt; \ --cert /etc/kubernetes/pki/etcd/peer.crt \ --key /etc/kubernetes/pki/etcd/peer.key \ --cacert /etc/kubernetes/pki/etcd/ca.crt Pod同理</description></item><item><title>k8s使用自定义的指标进行hpa</title><link>dingtalk.pub/post/2055568/</link><pubDate>Fri, 04 Dec 2020 06:03:01 +0000</pubDate><guid>dingtalk.pub/post/2055568/</guid><description>介绍 Horizontal Pod Autoscaling，简称HPA，是Kubernetes中实现POD水平自动伸缩的功能。为什么要水平而不叫垂直, 那是因为自动扩展主要分为两种:水平扩展(scale out)，针对于实例数目的增减垂直扩展(scal up)，即单个实例可以使用的资源的增减, 比如增加cpu和增大内存而HPA属于前者。它可以根据CPU使用率或应用自定义metrics自动扩展Pod数量(支持 replication controller、deployment 和 replica set) 实施过程 部署prometheus 自定义的指标需要从集群中抓取，而这些指标的抓取需要prometheus的支持，这里推荐一个最佳实践，基于prometheus-operator的全栈k8s监控项目： kube-prometheus ,部署该项目后，集群将拥有全栈的监控功能，为下一步，部署自定义的指标做准备 部署prometheus-adapter 该组件，提供了自定义指标的功能，用于将prometheus中的指标抓取为api资源，以便在hpa的定义中使用抓取到的metrics，官方推荐的安装方式是这样的：#如果所在环境全局翻墙的话，这个办法倒是挺好的 $ helm install --name my-release stable/prometheus-adapter 但是，由于国内环境的原因，我们可以把该资源导出为yaml文件，以便在本地部署或修改；在能够通外网的vps上操作一波#全栈的helm工具，需要翻墙，在本地使用的话，我们可以仅仅初始化helm客户端 helm init --client-only #将模板下载到本地 helm fetch --untar --untardir . stable/prometheus-adapter #使用模板生成配置文件 helm template prometheus-adapter --set prometheus.url=http://prometheus.monitoring.svc &amp;gt; prometheus-adapter.yaml #这样，就生成了本地配置文件prometheus-adapter.yaml，image，镜像字段必须要修改为国内可以下载到的；修改其中的字段或者其他值，可以参考文末链接 使用生成的yaml本地文件，在墙内的服务器上执行kubectl apply -f prometheus-adapter.yaml #查看自定义的指标是否生效 kubectl get --raw &amp;#34;/apis/custom.metrics.k8s.io/&amp;#34; | jq kubectl get --raw &amp;#34;/apis/custom.metrics.k8s.io/v1beta1/&amp;#34; | jq #如果有数据返回的话，就已经可以使用了 参考：Prometheus Adapter</description></item><item><title>LVM扩容实践</title><link>dingtalk.pub/post/1530017/</link><pubDate>Thu, 03 Dec 2020 05:49:37 +0000</pubDate><guid>dingtalk.pub/post/1530017/</guid><description>背景 在虚拟化平台上，为了方便资源分配，磁盘和内存都是按需分配的，内存分配后可以直接生效，磁盘扩容需要配置一定的操作实现扩容lvm LVM介绍 图中顶部，首先是实际的物理磁盘及其划分的分区和其上的物理卷（PV）。一个或多个物理卷可以用来创建卷组（VG）。然后基于卷组可以创建逻辑卷（LV）。只要在卷组中有可用空间，就可以随心所欲的创建逻辑卷。文件系统就是在逻辑卷上创建的，然后可以在操作系统挂载和访问。 实践  传统的LVM扩容方法，是通过添加新的磁盘或者磁盘分区来建立新的PV，再将PV加入到VG中，从而扩大VG空间，再对相关的LV进行扩容，最后增加文件系统空间完成整个扩容工作。这种方法比较适合使用本地存储的物理机，因为磁盘空间都是固定的，只能通过增加硬盘来增加磁盘空间，这种方法可以在线完成，无需停机重启。但扩容多次后，VG中会存在多个PV，磁盘逻辑结构变得复杂，容易增加日后维护存储和磁盘分区布局的难度 添加硬盘-&amp;gt;将新硬盘做成pv-&amp;gt;将pv加入vg-&amp;gt;让lv使用vg新增的空间-&amp;gt;刷新文件系统，使得lv的扩容生效 具体操作如下 方案一 #1.查看分区分区  fdisk -l #2.创建物理卷  pvcreat /dev/sdb1 #3.查看卷组名称及使用情况 vgdisplay #4.将物理卷扩展到卷组  vgextend centos /dev/sdb (此处‘centos’是卷组名称) #5.查看要扩展的逻辑卷 lvdisplay #得到要扩展的lv为 /dev/centos/root #6.将卷组中空闲空间扩展到 /  #lvextend -l +100%FREE /dev/centos/root #lvextend -L +50G /dev/ubuntu-vg/ubuntu-lv 6.刷新文件系统是扩容生效，根据文件系统的不同命令不同 df -hT 查看文件系统类型 #ext4使用如下命令 e2fsck -f /dev/centos/root resize2fs /dev/centos/root #xfs使用如下命令 xfs_growfs /dev/centos/root 方案二 在方案一上面的改变是，不直接增加新的硬盘；由于在虚拟化环境中可以直接增加磁盘容量，所以这里选择直接增加磁盘容量，而不是添加硬盘，个人认为在虚拟化环境下更方便管理 修改原硬盘大小-&amp;gt;在原磁盘新建分区-&amp;gt;将新分区做成pv-&amp;gt;将pv加入vg-&amp;gt;让lv使用vg新增的空间-&amp;gt;刷新文件系统，使得lv的扩容生效具体操作如下 #1.直接编辑虚拟机资源，在原硬盘上，修改硬盘容量到目标值 #2.重新扫描硬件，使系统识别到变化 echo '1' &amp;gt; /sys/class/scsi_disk/0\:0\:0\:0/device/rescan echo '1' &amp;gt; /sys/class/scsi_disk/0\:0\:1\:0/device/rescan #3.查看新的大小是否被识别 fdisk -l #4.</description></item><item><title>基于helm的operators SDK</title><link>dingtalk.pub/post/16367432/</link><pubDate>Mon, 23 Nov 2020 03:41:57 +0000</pubDate><guid>dingtalk.pub/post/16367432/</guid><description>先决条件 安装operator sdk及其必备组件。具有Kubernetes v1.16.0+集群。具有群集管理权限的用户。 安装 brew install operator-sdk 快速步骤 创建项目 创建并更改为项目的目录。然后使用Helm插件调用operator-sdk init来初始化基础项目布局：
mkdir nginx-operator cd nginx-operator operator-sdk init --plugins=helm #模板，会自动创建chart # operator-sdk init --plugins=helm --domain=com --group=example --version=v1alpha1 --kind=Nginx # Use an existing chart # 创建一个 API 使用Helm的内置的chart 模板 创建一个简单的nginx API（来自helm create）：
operator-sdk create api --group demo --version v1 --kind Nginx Build and push the operator image 使用内置的Makefile目标来构建和推送operator。调用make时，需要定义IMG：
make docker-build docker-push IMG=imdingtalk/nginx-operator:0.1 运行 operator # Install CRDs into a cluster nginx-operator$make install /usr/local/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.</description></item><item><title>✔️ go,一天一个小碧池</title><link>dingtalk.pub/post/6758741/</link><pubDate>Mon, 23 Nov 2020 03:19:56 +0000</pubDate><guid>dingtalk.pub/post/6758741/</guid><description>How to reset and retrieve forgotten Harbor admin password (76289) https://kb.vmware.com/s/article/76289 device is busy 的处理方法 [root@biao yum.repos.d]# umount /mnt/cdrom/ umount: /mnt/cdrom: device is busy. [root@biao yum.repos.d]# fuser -m /mnt/cdrom/ ##查看在用的用户 /mnt/cdrom/: 1332c 4444c //占用进程pid,找到这个kill掉就欧克 ##或者查看后直接kill [root@biao yum.repos.d]fuser -m -k /mnt/cdrom/ TCP: time wait bucket table overflow #引起该现象的原因是服务器tcp的连接数太多，超出了内核定义的最大数 #vim /etc/sysctl.conf net.ipv4.tcp_max_tw_buckets = 20000# 过大也可能拖死服务器，关键还是要找出大量TW的原因，从根本上处理 # sysctl -p #一般原因为客户端没有主动关闭连接 #查看当前tcp的连接状态 [root@node1 ~]# netstat -n | awk &amp;#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&amp;#39; ESTABLISHED 181 TIME_WAIT 12 SYN_SENT 3 #找出80端口tcp请求最高的5个IP [root@node1 ~]# netstat -anlp | grep 80 | grep tcp | awk &amp;#39;{print $5}&amp;#39; | awk -F: &amp;#39;{print $1}&amp;#39; | sort | uniq -c | sort -nr | head -n5 6 127.</description></item><item><title>集群升级导致helm无法直接升级的问题处理</title><link>dingtalk.pub/post/16022662/</link><pubDate>Mon, 16 Nov 2020 06:07:04 +0000</pubDate><guid>dingtalk.pub/post/16022662/</guid><description>背景 之前一直正常在用的测试环境，k8s版本升级，使用helm upgrade升级应用，报错
Error: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: unable to recognize &amp;#34;&amp;#34;: no matches for kind &amp;#34;Deployment&amp;#34; in version &amp;#34;apps/v1beta2&amp;#34; helm.go:75: [debug] unable to recognize &amp;#34;&amp;#34;: no matches for kind &amp;#34;Deployment&amp;#34; in version &amp;#34;apps/v1beta2&amp;#34; 处理 https://helm.sh/docs/topics/kubernetes_apis/
由于新版本集群，弃用了部分api，所以helm对应的需要更新模板，并且更新现有的manifest
更新模板，更新现有的helm模板中的api版本 使用新的模板更新应用 helm upgrade ... 依旧会报错，需要更新现有的在运行的manifest
更新现有在运行的manifest中的api版本,直接使用helm的插件更新 helm plugin install https://github.com/hickeyma/helm-mapkubeapis helm get manifest RELEASE helm mapkubeapis RELEASE 再次使用 helm upgrade .</description></item><item><title>logging-operator</title><link>dingtalk.pub/post/14758613/</link><pubDate>Thu, 22 Oct 2020 09:08:59 +0000</pubDate><guid>dingtalk.pub/post/14758613/</guid><description>项目地址：https://github.com/banzaicloud/logging-operator Overview Architecture You can define outputs (destinations where you want to send your log messages, for example, Elasticsearch, or and Amazon S3 bucket), and flows that use filters and selectors to route log messages to the appropriate outputs. You can also define cluster-wide outputs and flows, for example, to use a centralized output that namespaced users cannot modify.You can configure the Logging operator using the following Custom Resource Descriptions.</description></item><item><title>postgres</title><link>dingtalk.pub/post/13509091/</link><pubDate>Tue, 22 Sep 2020 02:49:56 +0000</pubDate><guid>dingtalk.pub/post/13509091/</guid><description>备份 #直接备份 DATE=`date +%Y-%m-%d` for w in `echo reach_from_ooredoo_0825`; do pg_dump ${w} |gzip &amp;gt; /backup/$DATE-${w}.gz;done #压缩备份 DATE=`date +%Y-%m-%d` for w in `echo ac db1 db2 `; do pg_dump -U target -h xxx.com ${w} | gzip &amp;gt; /backup//$DATE-${w}.gz; done 恢复 #压缩文件恢复 nohup gunzip -c reach.gz | psql -U postgres -h xxx &amp;lt;dbname&amp;gt; &amp;gt; nohup.log 2&amp;gt;&amp;amp;1 &amp;amp; #直接恢复 psql -U postgres -h xxx -f xxx.sql -d &amp;lt;dbname&amp;gt; --set ON_ERROR_STOP=ON 其他 #查看数据库size select pg_size_pretty(pg_database_size(&amp;#39;dbname&amp;#39;));</description></item><item><title>kubectl 无法自动补全之痛</title><link>dingtalk.pub/post/12647806/</link><pubDate>Thu, 03 Sep 2020 17:06:09 +0000</pubDate><guid>dingtalk.pub/post/12647806/</guid><description>背景 莫名的，kubectl无法自动补全了，总是报错没有啥bb函数，网上各种搜罗，都没有找到解决的办法，重装bash-complete多次无果，奇怪得很，万般无奈。。始终报错bash: completion: function __start_kubectl' not found·最终还是解决了 helm3$kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;16&amp;#34;, GitVersion:&amp;#34;v1.16.3&amp;#34;, GitCommit:&amp;#34;b3cbbae08ec52a7fc73d334838e18d17e8512749&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2019-11-13T11:23:11Z&amp;#34;, GoVersion:&amp;#34;go1.12.12&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;} Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;15+&amp;#34;, GitVersion:&amp;#34;v1.15.12-gke.9&amp;#34;, GitCommit:&amp;#34;0bfb4b7e4478e9ccb67f3c55ce3a5eb20d3bb586&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-06-26T16:49:38Z&amp;#34;, GoVersion:&amp;#34;go1.12.17b4&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} 根据kubectl的自带帮助操作了一把，就可以了，官方文档不是这么写的，有点小坑
source &amp;lt;(kubectl completion bash) ## Write bash completion code to a file and source if from .bash_profile kubectl completion bash &amp;gt; ~/.kube/completion.bash.inc printf &amp;#34; # Kubectl shell completion source &amp;#39;$HOME/.kube/completion.bash.inc&amp;#39; &amp;#34; &amp;gt;&amp;gt; $HOME/.bash_profile source $HOME/.bash_profile</description></item><item><title>嗯~每天少玩一会手机</title><link>dingtalk.pub/post/2796357/</link><pubDate>Tue, 04 Aug 2020 07:49:35 +0000</pubDate><guid>dingtalk.pub/post/2796357/</guid><description>大概每天更新一下吧 我觉得，每天手机亮屏的时间不超过一个小时，才算是正常的呀~</description></item><item><title>部署 GlusterFS 存储服务端</title><link>dingtalk.pub/post/5928345/</link><pubDate>Tue, 04 Aug 2020 07:48:34 +0000</pubDate><guid>dingtalk.pub/post/5928345/</guid><description>介绍 GlusterFS 是一个开源的分布式文件系统，本指南将介绍如何在 centos系统部署一个节点数为 3 的 GlusterFS 存储服务端集群和 Heketi，Heketi 用来管理 GlusterFS，并提供 RESTful API 接口供 Kubernetes 调用。正式环境搭建 GlusterFS 集群请参考 是一个开源的分布式文件系统，本指南将介绍如何在 Ubuntu 系统部署一个节点数为 2 的 GlusterFS (v3.12.12) 存储服务端集群和 Heketi，Heketi 用来管理 GlusterFS，并提供 RESTful API 接口供 Kubernetes 调用。本指南仅供测试 KubeSphere 存储服务端的搭建，正式环境搭建 GlusterFS 集群请参考 GlusterFS 官方网站，搭建 Heketi 请参考 官方文档。安装 #安装源 yum install centos-release-gluster #安装GlusterFS yum install glusterfs-server #启动GlusterFS管理守护程序 systemctl enable glusterd systemctl start glusterd #配置信任池 #在server1上执行 gluster peer probe server2 #在server2上执行 gluster peer probe server1 #至此，集群已经完成 #这里可以手动测试 #设置GlusterFS卷 mkdir /bricks/brick1/gv0 gluster volume create gv0 replica 2 server1:/bricks/brick1/gv0 server2:/bricks/brick1/gv0 gluster volume start gv0 gluster volume info 测试  mount -t glusterfs server1:/gv0 /mnt for i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done ls -lA /mnt | wc -l ls -lA /bricks/brick1/gv0 heketi  安装和初始化 #安装 [root@node1 heketi]# yum install heketi # heketi配置文件 /etc/heketi/heketi.</description></item><item><title>win10 2004 使用wsl2</title><link>dingtalk.pub/post/7957457/</link><pubDate>Sat, 06 Jun 2020 10:56:12 +0000</pubDate><guid>dingtalk.pub/post/7957457/</guid><description>安装适用于 Linux 的 Windows 子系统  必须先启用“适用于 Linux 的 Windows 子系统”可选功能，然后才能在 Windows 上安装 Linux 分发版以管理员身份打开 PowerShell 并运行： dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart 更新到 WSL 2 若要更新到 WSL 2，必须满足以下条件：
运行 Windows 10（已更新到版本 2004 的内部版本 19041 或更高版本）。 通过按 Windows 徽标键 + R，检查你的 Windows 版本，然后键入 winver，选择“确定”。 （或者在 Windows 命令提示符下输入 ver 命令）。 确保内部版本不低于 19041  启用“虚拟机平台”可选组件 安装 WSL 2 之前，必须启用“虚拟机平台”可选功能。 以管理员身份打开 PowerShell 并运行： dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 重新启动计算机，以完成 WSL 安装并更新到 WSL 2。 更新 WSL 2 Linux 内核 下载 Linux 内核更新包,下载适用于 x64 计算机的最新 WSL2 Linux 内核更新包 将 WSL 2 设置为默认版本 安装新的 Linux 分发版时，请在 Powershell 中运行以下命令，以将 WSL 2 设置为默认版本： wsl --set-default-version 2 将分发版版本设置为 WSL 1 或 WSL 2 查看当前的Linux使用的wsl版本 wsl --list --verbose 若要将分发版设置为受某一 WSL 版本支持，运行：</description></item><item><title>记一次tls某些客户端访问极慢的问题</title><link>dingtalk.pub/post/6797397/</link><pubDate>Fri, 08 May 2020 15:04:20 +0000</pubDate><guid>dingtalk.pub/post/6797397/</guid><description>问题 公司使用的let&amp;rsquo;s encrypt的通配证书用于公司的网站全站https化，最近反馈以下问题：
win系统访问</description></item><item><title>pvc扩容实践</title><link>dingtalk.pub/post/4986808/</link><pubDate>Thu, 07 May 2020 14:50:16 +0000</pubDate><guid>dingtalk.pub/post/4986808/</guid><description>准备工作 参考：https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims 前提 需要一个ceph集群 需要开启sc的allowVolumeExpansion  从k8s v1.11 版本开始支持
kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: rbd provisioner: ceph.com/rbd parameters: monitors: 172.16.13.44:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: &amp;#34;2&amp;#34; imageFeatures: layering allowVolumeExpansion: true 制作镜像  制作 kube-controller-manager 镜像，增加rbd命令 from centos:7 COPY kube-controller-manager /usr/bin/kube-controller-manager RUN mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup COPY Centos-7.repo /etc/yum.repos.d/Centos-7.repo COPY epel-7.repo /etc/yum.repos.d/epel-7.repo COPY ceph.repo /etc/yum.repos.d/ceph.repo COPY ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring COPY ceph.</description></item><item><title>Mysql 8</title><link>dingtalk.pub/post/6368447/</link><pubDate>Thu, 07 May 2020 13:17:40 +0000</pubDate><guid>dingtalk.pub/post/6368447/</guid><description>初始化 更新密码 #查找默认密码 systemctl status mysqld.service -l 或者 grep &amp;#39;temporary password&amp;#39; /var/log/mysqld.log #登录 mysql -uroot -p #修改默认密码 mysql&amp;gt; ALTER USER &amp;#39;root&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;*{your-password}*&amp;#39;; #创建用户 create user &amp;#39;news&amp;#39;@&amp;#39;39.15.16.14&amp;#39; identified by &amp;#39;123news&amp;#39;; #授权 grant all privileges on news.* to &amp;#39;news&amp;#39;@&amp;#39;39.15.16.14&amp;#39;; flush privileges; 忘记密码 # 参考 https://dev.mysql.com/doc/refman/8.0/en/resetting-permissions.html</description></item><item><title>https的通信过程,TLS1.2</title><link>dingtalk.pub/post/2525675/</link><pubDate>Wed, 06 May 2020 06:41:37 +0000</pubDate><guid>dingtalk.pub/post/2525675/</guid><description>HTTPS简介 HTTPS其实是有两部分组成：HTTP + SSL / TLS，也就是在HTTP上又加了一层处理加密信息的模块。服务端和客户端的信息传输都会通过TLS进行加密，所以传输的数据都是加密后的数据 过程解析 简单来说，是这样的：浏览器发起请求-&amp;gt;服务端返回证书-&amp;gt;浏览器使用证书（其中包含服务端的公钥）加密一段信息（pre master key + 握手信息）发给服务器-&amp;gt;服务器使用私钥和计算出的对称密钥解密-&amp;gt;服务器使用对称密码体系发送信息给浏览器。握手完成。实际上，其中还有一些复杂的东东：以上就是SSL的四次通信-&amp;gt; 客户端向服务端发送请求，包含一个随机数C,稍后用于生成&amp;quot;对话密钥&amp;quot;。
支持的协议版本，比如TLS 1.1版 一个客户端生成的随机数，稍后用于生成&amp;quot;对话密钥&amp;quot;。 支持的加密方法（用于加密随后的pre master key 和生成对称密钥?），比如RSA公钥加密。 支持的压缩算法等 -&amp;gt; 服务端返回数字证书和选定的加密算法和hash算法以及一个随机数S,稍后用于生成&amp;quot;对话密钥&amp;quot;。
确认使用的加密通信协议版本，比如TLS 1.1版本。如果浏览器与服务器支持的版本不一致，服务器关闭加密通信。 一个服务器生成的随机数，稍后用于生成&amp;quot;对话密钥&amp;quot;。 确认使用的加密方法，比如RSA公钥加密；以及服务器的证书 -&amp;gt; 客户端用自己的CA[主流的CA机构证书一般都内置在各个主流浏览器中]公钥去解密证书,如果证书有问题会提示风险， 如果证书没问题客户端会生成一个随机数（pre-master key）结合随机数S和随机数C来计算出一个对称加密的密钥（enc_key=Fuc(random_C, random_S, Pre-Master）,pre-master key 将使用服务器的公钥加密；结合之前所有发送的信息和相关信息计算一个hash和一些其他信息，使用对称加密加密数据(不会发送密钥)，发给服务端；并发送编码改变通知
一个随机数。该随机数用服务器公钥加密，防止被窃听。 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送  客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供服务器校验。 -&amp;gt; 服务器收到加密信息后，使用私钥解密（可以验证服务器端有正确的私钥）出pre-master key ， 然后结合随机数S和随机数C来计算出一个对称加密的密钥，使用这个密钥解密对称加密后的信息，其中包含hash值；同时服务器端会计算之前所有收到的信息和相关信息的hash值。这两个值做对比，以确保双方可以使用协商好的对称密钥和算法加解密信息（同时也再次确认了服务器端有正确私钥）。这一项同时也是前面发送的所有内容的hash值，用来供客户端校验。并发送编码改变通知-&amp;gt; 客户端解密接收后的信息，验证加密信息中的hash值和自己使用前面接收到的信息计算的hash值做对比，一致则验证通过，之后双方就拿着这个对称加密秘钥来进行正常的通信RSA密钥交换 称其为RSA密钥交换实际上是不准确的。 好吧，RSA使用非对称加密来创建会话密钥。公钥/私钥对扮演着重要角色。它在过程中：
客户端和服务器交换两个素数（x和y），称为随机数。 客户端生成一个主密码（a），然后使用服务器的公钥对其进行加密并将其发送给服务器。 服务器使用相应的私钥解密预主密钥，双方现在都具有所有三个输入，并将它们与某些伪随机函数（PRF）混合以生成主密钥。 双方将更多的PRF与主密钥混合在一起，并得出匹配的会话密钥。 但是在实际应用中，已经很少使用RSA作为密钥交换算法 TLS 1.2 握手过程 – Diffie-Hellman 密钥交换 与RSA一样，客户端以“ ClientHello”消息开头，该消息包括密码套件列表以及客户端随机数。 服务器以自己的“ ServerHello”消息作为响应，该消息包括其选定的密码套件和服务器随机数。 服务器发送其SSL证书，就像使用RSA TLS握手一样，客户端将运行一系列检查以验证证书是否有效，但是由于DH本身无法验证服务器，因此需要其他机制。 为了提供身份验证，服务器将获取客户机和服务器的随机数以及将用于计算会话密钥的DH参数，并使用其私钥对其进行加密。此功能用作数字签名，客户端将使用公钥来验证签名-并且服务器是密钥对的合法所有者-并使用其自己的DH参数进行响应。 服务器以“Server Hello Done”消息结束此往返。 与RSA不同，客户端不需要使用非对称加密将主密码前的机密发送到服务器，而是客户端和服务器使用之前交换的DH参数来获取pre-master secret。然后，每个用户都使用它刚刚计算出的 pre-master secret来计算会话密钥。 客户端发送“Change Cipher Spec”消息，以通知另一方其已切换到加密。 客户端发送最后一条“Finished”消息，以表明它已完成握手的一部分。 同样，服务器发送“Change Cipher Spec”消息。 握手以服务器“Finished”消息结束。 参考 https://www.</description></item><item><title>Nginx TLS1.3 实践</title><link>dingtalk.pub/post/6653199/</link><pubDate>Tue, 05 May 2020 05:06:00 +0000</pubDate><guid>dingtalk.pub/post/6653199/</guid><description>TLS1.3 优势 在性能方面，TLS 1.2需要两次往返来建立HTTPS连接。使用TLS 1.3，仅需要一次往返。TLS 1.3还支持零往返模式  在安全性方面，TLS 1.3删除了对旧密码套件的支持 在Nginx中启用TLS 1.3  要使用Nginx启用TLS 1.3，有两个要求。
Nginx版本必须支持TLS 1.3。这需要nginx 1.13或更高。 Nginx需要使用OpenSSL 1.1.1+构建或与OpenSSL 1.1.1+一起运行。  这里直接使用三方源来安装 yum install &amp;#34;https://packages.exove.com/yum/el7/exove-centos-release.el7.noarch.rpm&amp;#34; yum update nginx [root@biao ~]# nginx -V nginx version: nginx/1.18.0 (packages.exove.com: SSE2, openssl-1.1.1g, PCRE JIT, TCP Fast Open) built by gcc 8.3.1 20190311 (Red Hat 8.3.1-3) (GCC) built with OpenSSL 1.1.1g 21 Apr 2020 TLS SNI support enabled 开启配置 server { listen 443 ssl; root /opt/www/tls; ssl_certificate /opt/www/tls/server.</description></item><item><title>TLS1.3 初识，握手过程和优化</title><link>dingtalk.pub/post/6653455/</link><pubDate>Tue, 05 May 2020 05:04:04 +0000</pubDate><guid>dingtalk.pub/post/6653455/</guid><description>更快，更安全  TLS1.3的最终版本已于2018.8月发布（RFC8446）对于TLS 1.2，需要两次往返来完成TLS握手。在1.3版本中，它仅需要一次往返，_ _从而将加密延迟减少了一半。这有助于使这些加密的连接比以前更灵活 TLS 1.3浏览器支持 从Chrome 65 开始，Chrome在beta支持，在Chrome 70（于2018年10月发布）中，TLS 1.3的最终版本已启用，用于发出连接请求（即首次请求会申明我支持TLS1.3）。 在Firefox 52及更高版本（包括Quantum）中启用了TLS 1.3的beta版本。 Firefox 63（于2018年10月发布）的,支持最终版本为TLS 1.3。 Microsoft Edge开始开始在76版本支持TLS 1.3，并且默认情况下在macOS 10.14.4的Safari 12.1中启用了该功能。  相对于TLS1.2的变化  消除了对过时的算法和密码的支持 消除RSA密钥交换，要求完善的前向保密性 减少握手中的协商次数 将密码套件中的算法数量减少到2 消除了块模式密码并强制执行AEAD批量加密 使用HKDF密码提取和密钥派生 提供1-RTT模式和零往返恢复 签名整个握手协议，对TLS 1.2进行了改进 支持其他椭圆曲线 消除了易受攻击的算法和密码 时间一直都是并且永远都是任何加密系统的敌人，随着时间的流逝，发现特定加密系统的漏洞和对漏洞的利用，会给加密系统带来很大的风险，前向保密性保证了长期使用的主密钥泄漏不会导致过去的会话密钥泄漏，可以保证在主密钥泄露时历史通讯的安全，即使系统遭到主动攻击也是如此 简化密钥交换过程  一般来说，有两种流行的机制用于交换握手后将在HTTPS连接期间使用的安全会话密钥：
RSA Diffie-Hellman 新版在密钥交换期间已经完全弃用了RSA，实际上上在1.2版本中也很少使用RSA 这是OpenSSL现在支持的五个TLS 1.3密码套件。
TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_AES_128_GCM_SHA256 TLS_AES_128_CCM_8_SHA256 TLS_AES_128_CCM_SHA256 密码协商过程  TLS1.2 套件示例TLS 1.3对其前身进行了无数的改进，IETF取消了对较旧的过时算法的支持，并简化了所有流程，将整个握手过程从两次往返缩短为一次，并将密码套件的大小从四个协商/算法减少为两个(密钥交换算法始终未DHE或者ECDHE;)。支持的密码套件的数量也从37个减少到5个。这是TLS 1.3密码套件的示例：在hello时我们将知道使用某种版本的Diffie-Hellman临时密钥交换（密钥交换不需要协商了），我们只是不知道参数，因此这意味着不再需要TLS 1.2密码套件中的前两个算法（对称加密和认证算法）。这些功能仍在发生，而不再需要在握手期间进行协商，TLS 1.3允许的唯一对称密码类型称为带有附加数据的身份验证加密（AEAD）密码。 常见的 AEAD 算法如下：</description></item><item><title>k8s使用glusterFS做SC存储</title><link>dingtalk.pub/post/5953671/</link><pubDate>Thu, 23 Apr 2020 01:49:50 +0000</pubDate><guid>dingtalk.pub/post/5953671/</guid><description>准备 前提是需要准备好一个gluster集群，并提供rest url 应用 创建一个storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: gluster-vol-default provisioner: kubernetes.io/glusterfs parameters: resturl: &amp;#34;http://192.168.10.100:8080&amp;#34; restuser: &amp;#34;&amp;#34; secretNamespace: &amp;#34;&amp;#34; secretName: &amp;#34;&amp;#34; volumetype: &amp;#34;replicate:3&amp;#34; #允许卷扩展，PVC可以在使用的时候调整大小 #https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims allowVolumeExpansion: true --- apiVersion: v1 kind: Secret metadata: name: heketi-secret namespace: default data: # base64 encoded password. E.g.: echo -n &amp;#34;mypassword&amp;#34; | base64 key: bXlwYXNzd29yZA== type: kubernetes.io/glusterfs secret示例文件参考： https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs https://github.com/kubernetes/examples/blob/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-storageclass.yaml</description></item><item><title>quay,gcr镜像国内拉取</title><link>dingtalk.pub/post/6033915/</link><pubDate>Wed, 22 Apr 2020 10:25:06 +0000</pubDate><guid>dingtalk.pub/post/6033915/</guid><description>常用镜像仓库 DockerHub镜像仓库:https://hub.docker.com/阿里云镜像仓库：https://cr.console.aliyun.comgoogle镜像仓库：http://gcr.io/google-containers/http://gcr.io/kubernetes-helm/coreos镜像仓库：https://quay.io/repository/RedHat镜像仓库：https://access.redhat.com/containers docker.io 镜像加速  针对Docker客户端版本大于 1.10.0 的用户可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&amp;#39;EOF&amp;#39; { &amp;#34;registry-mirrors&amp;#34;: [&amp;#34;https://d0rzu7aw.mirror.aliyuncs.com&amp;#34;] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 或者使用中科大的镜像 docker pull docker.mirrors.ustc.edu.cn/library/mysql:5.7 gcr.io镜像  推荐使用阿里云的镜像 #示例 docker pull k8s.gcr.io/pause:3.2 #改为 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.18.0 微软云的镜像本来很给力，但是后面不开放给公共使用了 说明参考这个链接： http://mirror.azure.cn/help/gcr-proxy-cache.html quay.io镜像加速  #示例 docker pull quay.io/xxx/yyy:zzz #改为 docker pull quay.mirrors.ustc.edu.cn/xxx/yyy:zzz 以拉取quay.io/coreos/kube-state-metrics:v1.5.0为例，如下： docker pull quay.mirrors.ustc.edu.cn/coreos/kube-state-metrics:v1.5.0 使用代理 要是所有的仓库都不给力的话，那就使用自己的代理咯 vim /etc/systemd/system/docker.service.d/docker-proxy.conf [Service] Environment=&amp;#34;HTTP_PROXY=http://172.16.8.244:10809/&amp;#34; &amp;#34;HTTPS_PROXY=http://172.16.8.244:10809/&amp;#34; &amp;#34;NO_PROXY=localhost,127.</description></item><item><title>k8s零宕机滚动更新</title><link>dingtalk.pub/post/6306716/</link><pubDate>Tue, 21 Apr 2020 03:42:00 +0000</pubDate><guid>dingtalk.pub/post/6306716/</guid><description>滚动更新 默认情况下，Kubernetes 的 Deployment 是具有滚动更新的策略来进行 Pod 更新的，该策略可以在任何时间点更新应用的时候保证某些实例依然可以正常运行来防止应用 down 掉，当新部署的 Pod 启动并可以处理流量之后，才会去杀掉旧的 Pod。下面示例是使用默认的滚动更新升级策略的一个 Deployment 定义，在更新过程中最多可以有一个超过副本数的容器（maxSurge），并且在更新过程中没有不可用的容器。 apiVersion: apps/v1 kind: Deployment metadata: labels: app: testready name: testready spec: replicas: 2 selector: matchLabels: app: testready template: metadata: labels: app: testready spec: containers: - image: nginx:1.12 name: nginx readinessProbe: exec: command: - grep - &amp;#34;jvm&amp;#34; - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: labels: app: testready name: testready spec: ports: - name: &amp;#34;80&amp;#34; port: 80 protocol: TCP targetPort: 80 selector: app: testready type: NodePort 观察pod的就绪状态及服务ep状态</description></item><item><title>chrome离线安装包下载</title><link>dingtalk.pub/post/1429810/</link><pubDate>Thu, 02 Apr 2020 03:42:12 +0000</pubDate><guid>dingtalk.pub/post/1429810/</guid><description>如果您要为单个用户帐户安装Google Chrome：下载Google Chrome离线安装程序（32位）&amp;mdash;&amp;gt;国内点这里下载Google Chrome离线安装程序（64位）&amp;mdash;&amp;gt;国内点这里如果您要为所有用户帐户安装Google Chrome：适用于所有用户帐户的Google Chrome离线安装程序（32位）&amp;mdash;&amp;gt;国内点这里适用于所有用户帐户的Google Chrome离线安装程序（64位）&amp;mdash;&amp;gt;国内点这里2019.3.30 当前最新版本： 73.0.3683.86 国内共享文件解压密码为： areuok历史版本下载，处于安全原因，谷歌不提供官方历史版本下载，推荐使用对应的chromium 参考： https://www.chromium.org/getting-involved/download-chromium 历史chromium下载地址推荐： https://chromium.cypress.io/</description></item><item><title>ceph管理相关</title><link>dingtalk.pub/post/4944055/</link><pubDate>Fri, 06 Mar 2020 08:56:02 +0000</pubDate><guid>dingtalk.pub/post/4944055/</guid><description>删除一个pool #删除一个存储池 其实执行代码： ceph osd pool delete test3 test3 –yes-i-really-really-mean-it 就可以完成删除，但是此时有时候会报错： Error EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool 这是由于没有配置mon节点的 mon_allow_pool_delete 字段所致，解决办法就是到mon节点进行相应的设置。 解决方案： 注：1-3步的操作必须在mon节点上执行 打开mon节点的配置文件： [root@node1 ceph]# vim /etc/ceph/ceph.conf 在配置文件中添加如下内容： [mon] mon_allow_pool_delete = true 重启ceph-mon服务： [root@node1 ceph]# systemctl restart ceph-mon.target 执行删除pool命令： [root@node3 ~]# ceph osd pool delete test3 test3 –yes-i-really-really-mean-it pool ‘ecpool’ removed POOL_APP_NOT_ENABLED #查看详细信息 [root@biao tmp]# ceph health detail HEALTH_WARN application not enabled on 1 pool(s); mon biao is low on available space POOL_APP_NOT_ENABLED application not enabled on 1 pool(s) application not enabled on pool 'kube' use 'ceph osd pool application enable &amp;lt;pool-name&amp;gt; &amp;lt;app-name&amp;gt;', where &amp;lt;app-name&amp;gt; is 'cephfs', 'rbd', 'rgw', or freeform for custom applications.</description></item><item><title>k8s使用ceph做storageclass存储</title><link>dingtalk.pub/post/4906138/</link><pubDate>Tue, 03 Mar 2020 07:19:02 +0000</pubDate><guid>dingtalk.pub/post/4906138/</guid><description>k8s内建支持ceph RBD  参考配置： https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd官方参考配置文件 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/rbd parameters: monitors: 10.16.153.105:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: &amp;#34;2&amp;#34; imageFeatures: &amp;#34;layering&amp;#34; 需要一个admin和user的secret #Create a Ceph admin secret ceph auth get client.admin 2&amp;gt;&amp;amp;1 |grep &amp;#34;key = &amp;#34; |awk &amp;#39;{print $3&amp;#39;} |xargs echo -n &amp;gt; /tmp/key kubectl create secret generic ceph-admin-secret --from-file=/tmp/key --namespace=kube-system --type=kubernetes.io/rbd #Create a Ceph pool and a user secret ceph osd pool create kube 8 8 ceph auth add client.</description></item><item><title>ceph部署相关</title><link>dingtalk.pub/post/4223814/</link><pubDate>Tue, 03 Mar 2020 03:28:26 +0000</pubDate><guid>dingtalk.pub/post/4223814/</guid><description>安装部署 首先安装ceph-deploy yum install https://download.ceph.com/rpm-luminous/el7/noarch/ceph-deploy-2.0.0-0.noarch.rpm 创建一个单节点集群  参考： https://docs.ceph.com/docs/master/start/quick-ceph-deploy/ 首先创建配置文件 mkdir myceph cd myceph ceph-deploy new {initial-monitor-node(s)} 因为我们是单节点ceph集群，因此需要将集群的副本数量设置为1，这样方便一些。具体方法是把如下内容加入到 ceph.conf 里面。
[global] osd pool default size = 1 osd pool default min size = 1 安装指定版本软件包（此处安装较慢，所以必须使用国内的源） 首先指明在安装的时候使用国内的源 sed -i 's#htt.*://download.ceph.com#https://mirrors.tuna.tsinghua.edu.cn/ceph#g' /etc/yum.repos.d/ceph.repo 申明变量的方式 （推荐）
export CEPH_DEPLOY_REPO_URL=https://mirror.tuna.tsinghua.edu.cn/ceph/rpm-nautilus/el7/ export CEPH_DEPLOY_GPG_URL=https://mirror.tuna.tsinghua.edu.cn/ceph/keys/release.asc 然后再安装
ceph-deploy install --release nautilus biao ## --release是指明版本，有哪些版本，可以在这里查看：https://mirror.tuna.tsinghua.edu.cn/ceph/ 初始化 ceph-deploy mon create-initial #将生成以下文件 ceph.client.admin.keyring ceph.bootstrap-mgr.keyring ceph.bootstrap-osd.keyring ceph.bootstrap-mds.keyring ceph.bootstrap-rgw.keyring ceph.bootstrap-rbd.keyring ceph.bootstrap-rbd-mirror.keyring #使用ceph-deploy复制配置文件和管理密钥到您的管理节点和你的Ceph的节点， #以便您可以使用ceph CLI，而无需每次执行命令指定监视地址和 ceph.client.admin.keyring ceph-deploy admin {ceph-node(s)} #12.</description></item><item><title>k8s集群新加节点</title><link>dingtalk.pub/post/4054943/</link><pubDate>Thu, 09 Jan 2020 02:45:52 +0000</pubDate><guid>dingtalk.pub/post/4054943/</guid><description>#如果是加入node节点 kubeadm token create --print-join-command #如果是新加入master节点 #确保初始化时候，上传了证书（经测试，好像不指定该参数也行） kubeadm init --control-plane-endpoint &amp;quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&amp;quot; --upload-certs #加入管理节点的token默认保留2小时，过期后可以重新生成 kubeadm init phase upload-certs --upload-certs #使用命令加入管理节点，相比node新增参数，--control-plane --certificate-key kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07</description></item><item><title>基于snmp的网络设备的监控.md</title><link>dingtalk.pub/post/3505113/</link><pubDate>Sat, 04 Jan 2020 08:07:43 +0000</pubDate><guid>dingtalk.pub/post/3505113/</guid><description>介绍 公司当前环境下有基于prometheus的监控体系，覆盖了所有的服务器和虚拟机，但是还没有包含网络设备，例如交换机或者防火墙，因此，需要补充网络设备监控部分
相关信息 prometheus： 监控系统，负责指标的收取和一些告警规则的定义snmp-exporter: 用于通过snmp协议暴露交换机的相关指标SNMP Exporter Config Generator： 此配置生成器使用NetSNMP解析MIB，并使用它们为snmp_exporter生成配置&amp;ndash;帮助生成snmp的配置文件MIB和OID： MIB是管理信息库的缩写，它是用于管理通信网络中的实体的数据库。数据库是分层的（树形结构），并且每个条目都通过对象标识符（OID）进行寻址snmp协议： SNMP 是专门设计用于在 IP 网络管理网络节点（服务器、工作站、路由器、交换机及HUBS等）的一种标准协议
部署 网络设备使能snmp agent system-view [Sysname] snmp-agent sys-info version v3 v2c //为了安全，最好只设置只读 [Sysname] snmp-agent community read xxxx ## 测试 yum -y install net-snmp-utils snmpwalk -v 2c（snmp的版本） -c xxxx(前面snmp-agent设置的密码) 10.10.1.1（交换机管理ip） .1.3.6.1.2.1.25.2.2（mib信息，需要查对应的厂家的mib信息库） #取得系统总内存 #如果能获取到返回信息，则snmp已经可以正常工作 使用snmp-exporter抓取snmp-agent的信息 snmp-exporter：https://github.com/prometheus/snmp_exporter使用snmp-exporter的关键是配置snmp-exporter的配置文件，默认有一个配置文件，但是里面涉及的信息基本都是国外交换机的配置，不能直接使用
snmp.yml 典型的一个配置如下（部分）
# WARNING: This file was auto-generated using snmp_exporter generator, manual changes will be lost. apcups: walk: - 1.3.6.1.2.1.2 - 1.</description></item><item><title>浪潮服务器安装esxi6.5报错nfs41client failed to load</title><link>dingtalk.pub/post/3888038/</link><pubDate>Sun, 29 Dec 2019 10:20:58 +0000</pubDate><guid>dingtalk.pub/post/3888038/</guid><description>背景 使用浪潮M5服务器安装esxi6.5版本，加载相关项过程中报错nfs41client failed to load 处理 百度了下，这个是网卡驱动的问题，服务器配置的是intel x722 for 1GB的网卡，百度了半天，使用百度的办法试了几次都不行，最后Google了一下 intel x722 driver esxi。。问题就解决了。怎么查找esxi的的兼容性驱动，Intel给了方法：https://www.intel.com/content/www/us/en/support/articles/000005693/network-and-io/ethernet-products.html 最终找到的驱动：https://my.vmware.com/cn/group/vmware/details?downloadGroup=DT-ESX65-INTEL-I40EN-195&amp;amp;productId=614
使用ESXI-customizer-v2.7.2（附件） 给默认的6.5镜像注入驱动，然后使用这个自定义的驱动再去安装就没有问题了
ESXi-Customizer-v2.7.2.rar
原因  新版的服务器使用的是新的网卡，老的镜像默认没有自带对新网络的驱动</description></item><item><title>centos更改网卡设置的名称</title><link>dingtalk.pub/post/3567002/</link><pubDate>Tue, 10 Dec 2019 08:14:15 +0000</pubDate><guid>dingtalk.pub/post/3567002/</guid><description>1.在 /etc/sysconfig/network-scirpts/ mv ifcfg-ens192 ifcfg-eth0 然后 vim ifcfg-eth0 把DEVICE改成 DEVICE=eth0 保存2. /etc/sysconfig/grub 下的 GRUB_COMLINE_LINUX=&amp;quot;&amp;quot; 中添加 net.ifnames=0 biosdevname=0 ” //禁用eno1*3. 执行 grub2-mkconfig -o /boot/grub2/grub.cfg 更新grub2文件4.reboot</description></item><item><title>influxdb相关的</title><link>dingtalk.pub/post/3549923/</link><pubDate>Mon, 09 Dec 2019 15:10:46 +0000</pubDate><guid>dingtalk.pub/post/3549923/</guid><description>介绍 基本概念  InfluxDB是一个由InfluxData开发的开源时序型数据。它由Go写成，着力于高性能地查询与存储时序型数据。InfluxDB被广泛应用于存储系统的监控数据，IoT行业的实时数据等场景 与传统数据库中的名词做比较  influxDB中的名词 传统数据库中的概念 database 数据库 measurement 数据库中的表 points 表里面的一行数据 Points Point由时间戳（time）、数据（field）、标签（tags）组成。 Point相当于传统数据库里的一行数据，如下表所示：
Point属性 传统数据库中的概念 time 每个数据记录时间，是数据库中的主索引(会自动生成) fields 各种记录值（没有索引的属性） tags 各种有索引的属性 安装 参考：https://portal.influxdata.com/downloads/ wget https://dl.influxdata.com/influxdb/releases/influxdb-1.7.9.x86_64.rpm sudo yum localinstall influxdb-1.7.9.x86_64.rpm 默认的安装信息 ll /usr/bin/influ* influxd # influxdb服务器 influx # influxdb命令行客户端 influx_inspect # 查看工具 influx_stress # 压力测试工具 influx_tsm # 数据库转换工具（将数据库从b1或bz1格式转换为tsm1格式） ll /var/lib/influxdb/ data # 存放最终存储的数据，文件以.</description></item><item><title>ELK相关的那些东东</title><link>dingtalk.pub/post/3415727/</link><pubDate>Sat, 30 Nov 2019 04:12:21 +0000</pubDate><guid>dingtalk.pub/post/3415727/</guid><description>部署 额，网上文章好多，后面在补充 使用相关 kibana  模糊查询日志 #模糊查询： log_message:boot* #模糊查询： log_message:&amp;#34;bootstrap false&amp;#34;~100 #表示两个单词之间最大匹配100个字符 开发工具的使用 console #获取ES基本信息，和之间访问 http://ES_IP:9200效果一样 GET / # 根据索引查询 GET INDEX_NAME.*/_search { &amp;#34;query&amp;#34;: { &amp;#34;bool&amp;#34;: { &amp;#34;must&amp;#34;: [ { &amp;#34;range&amp;#34;: { &amp;#34;@timestamp&amp;#34;: { &amp;#34;gte&amp;#34;: &amp;#34;now-10h&amp;#34;, &amp;#34;lte&amp;#34;: &amp;#34;now&amp;#34; } } }, { &amp;#34;match_phrase&amp;#34;: { &amp;#34;log_level&amp;#34;: &amp;#34;WARN&amp;#34; } }, { &amp;#34;match_phrase&amp;#34;: { &amp;#34;tags&amp;#34;: &amp;#34;DataWarehouse&amp;#34; } } ], &amp;#34;must_not&amp;#34;: [ { &amp;#34;match_phrase&amp;#34;: { &amp;#34;log_message&amp;#34;: &amp;#34;containermanager.ContainerManagerImpl (ContainerManagerImpl.java:handle(1654))&amp;#34; } }, { &amp;#34;match_phrase&amp;#34;: { &amp;#34;log_message&amp;#34;: &amp;#34;caught end of stream exception EndOfStreamException&amp;#34; } } ] } } } GROK DEBUG使用自定义表达式可以更精确的匹配日志信息 内置的表达式: https://github.</description></item><item><title>记一次虚拟机重启后网卡启动失败的问题，RTNETLINK answers: File exists</title><link>dingtalk.pub/post/2878743/</link><pubDate>Wed, 16 Oct 2019 14:41:08 +0000</pubDate><guid>dingtalk.pub/post/2878743/</guid><description>问题描述  在玩k8s的时候，改了一个api的静态配置文件，死活启动不起来静态pod了，api一直起不来，尝试万能重启大法，重启后，额，连接不上虚拟机了，登录VC ,发现，服务器的网卡没有IP 排查和处理 首先尝试，手动重启，么有效果  想到之前遇到过的坑，是不是/etc/udev/rules.d/70-persistent-net.rules  文件导致的，查看之，里面并没有有效的记录，所以，应该不是这个导致的 老实看日志吧 ,主要报错如下  #systemctl status network .... RTNETLINK answers: File exists RTNETLINK answers: File exists RTNETLINK answers: File exists .... 额，别人家的遇到这个可以像我的第二个想法那样处理了呀，为毛我不行，
疯狂ifup ifdown&amp;hellip;.然鹅并没有什么用 处理 再次检查网卡配置文件，，新添加了一行 ##在原配置文件新添加一行， DEVICE=eth0 #ifup eth0 ## 然后，尼玛，网卡就启动起来了，起来，，，了。 为毛我之前没有DEVICE这行，愉快重启那么多次么有问题？ WHY 在网卡启动过程中，新添加网卡的时候，识别的是DEVICE字段，而不是NAME字段（只是网络连接的名字），好比我的一个WiFi，无线网卡就那么一个，名字唯一，但是ssid可以有很多，随便娶。。我的配置文件之前是没有DEVICE字段的，导致/etc/udev/rules.d/70-persistent-net.rules 文件里面也没有对应的网卡信息，所以没得玩，而开机的时候需要从 ifcfg-eth0中获取DEVICE的名字，但是我没配置，所以没有启动起来网卡</description></item><item><title>NGINX禁止使用IP访问，防止恶意域名解析</title><link>dingtalk.pub/post/2857446/</link><pubDate>Tue, 15 Oct 2019 01:34:02 +0000</pubDate><guid>dingtalk.pub/post/2857446/</guid><description>什么是恶意域名解析 外部未知的域名持有者，将域名解析到非其所持有的服务器公网IP上，间接或直接造成损害他人利益的行为。 暗箭伤人 域名的恶意解析，可以用于借刀杀人。 这个手法很骚，轻则可以将对手的SEO排名拉低，重则可以让工信部封杀其站点。 **具体实现条件如下： **
未备案的域名或已被接入工信部黑名单的域名 获取要攻击的站点，其源服务器使用的公网IP 确认要攻击的网站80端口和443端口可以直接用IP直接访问 将黑域名解析到该公网IP **危害如下： **
不同域名解析到同个站点，真身域名权重被降低，SEO排名被假域名挤占  非法域名解析，导致源服务器被工信部封杀，网站停止服务 解决方法 将无效域名的HTTP请求，全部拒绝响应
新加一个server段如下： server { listen 443 default_server; listen 80 default_server; #筛选无效域名 server_name _; #直接返回错误码的方式 return 404; #rewrite 的方式(比较友好) #rewrite ^(.*)$ https://xxx.com/40x.html permanent; #日志可选，可以看看哪些非法访问 #access_log off; ssl_certificate &amp;#34;/usr/local/openresty/nginx/ssl/demofullchain.cer&amp;#34;; ssl_certificate_key &amp;#34;/usr/local/openresty/nginx/ssl/demo.dingtalk.pub.key&amp;#34;; }</description></item><item><title>k8s网络小探</title><link>dingtalk.pub/post/2809327/</link><pubDate>Fri, 11 Oct 2019 15:52:55 +0000</pubDate><guid>dingtalk.pub/post/2809327/</guid><description>容器网络 在说k8s的网络之前，先熟悉下容器的网络模式。
docker会在宿主机启动一张网卡 docker0 容器通过Veth Pair 连接容器和docker0，Veth Pair相当于网线 docker0 在容器通信的过程中相当于二层交换机 容器会有默认路由指向docker0 当一个容器试图连接到另外一个宿主机时，首先经过 docker0 网桥出现在宿主机上。然后根据宿主机路由规则路由 #使用以下命令查看网桥的连接情况 [root@rbtnode1 ~]# brctl show bridge name bridge id STP enabled interfaces br-5bc39396abe2 8000.024219350061 no veth1a289de veth3504d23 docker0 8000.0242d50f88b5 no veth210c0a2 ... [root@rbtnode1 ~]# ip ad | grep veth 16: vethwe-datapath@vethwe-bridge: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1376 qdisc noqueue master datapath state UP group default 17: vethwe-bridge@vethwe-datapath: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1376 qdisc noqueue master weave state UP group default 2631: veth210c0a2@if2630: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master docker0 state UP group default 2682: veth3504d23@if2681: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master br-5bc39396abe2 state UP group default ##这里可以看到我们宿主机存在一个虚拟网卡 veth3504d23 ，他一端连接的是docker0，另一端则连接的是容器的eth0 ##其他的容器在创建的时候也会发生同样的事 ##相当于启动容器后，会有一“网线”，连接docker0和容器，由于都连接同一个交换机，默认情况下，各个容器间可以直接互通 容器的通信示意图（源自张磊）：k8s网络的发展 在docker的默认配置下，容器的网络没法处理跨主机网络的互相通信。为了解决跨主机通信的问题，社区出现了一些方案 flannel UDP 模式 而 UDP 模式，是 Flannel 项目最早支持的一种方式，也是性能最差的一种模式，如今已被弃用，但是对我们理解集群的网络还是有帮助的 该模式的主要通信过程如下</description></item><item><title>aria2c 不限速下载服务配置 RPC</title><link>dingtalk.pub/post/2622524/</link><pubDate>Mon, 16 Sep 2019 07:28:28 +0000</pubDate><guid>dingtalk.pub/post/2622524/</guid><description>介绍 aria2是一个轻量级的多协议和多源，跨平台下载实用程序，在命令行中运行。它支持HTTP / HTTPS，FTP，SFTP，BitTorrent和Metalink。https://aria2.github.io/ 安装和使用 安装 在epel源有打包好的包可以直接安装
yum install aria2 -y 使用 Download from WEB: $ aria2c http://example.org/mylinux.iso Download from 2 sources: $ aria2c http://a/f.iso ftp://b/f.iso Download using 2 connections per host: $ aria2c -x2 http://a/f.iso BitTorrent: $ aria2c http://example.org/mylinux.torrent BitTorrent Magnet URI: $ aria2c &amp;#39;magnet:?xt=urn:btih:248D0A1CD08284299DE78D5C1ED359BB46717D8C&amp;#39; Metalink: $ aria2c http://example.org/mylinux.metalink Download URIs found in text file: $ aria2c -i uris.txt RPC的用法 使用rpc的方式可以扩展该下载工具为专用下载工具，可以被其他的下载软件调用，如 pandownload创建新的配置文件 vim ~/.aria2/aria2.conf ## &amp;#39;#&amp;#39;开头为注释内容, 选项都有相应的注释说明, 根据需要修改 ## ## 被注释的选项填写的是默认值, 建议在需要修改时再取消注释 ## ## 文件保存相关 ## # 文件的保存路径(可使用绝对路径或相对路径), 默认: 当前启动位置 dir=~/downloads # 启用磁盘缓存, 0为禁用缓存, 需1.</description></item><item><title>基于drone的CI-CD实践</title><link>dingtalk.pub/post/2315911/</link><pubDate>Mon, 02 Sep 2019 09:32:12 +0000</pubDate><guid>dingtalk.pub/post/2315911/</guid><description>介绍 组件 drone：自助式的CI/CD交互平台，提供开源版本，可私有部署 helm：k8s的包管理器 k8s：云原生.. gitlab：代码仓库 maven/dockerhub：镜像仓库 流程 安装 安装方式推荐docker方式，一条命令完事儿，drone需要安装drone-server和drone-agent，参考：https://docs.drone.io/installation/gitlab/ 安装server： docker run \ --volume=/var/run/docker.sock:/var/run/docker.sock \ --volume=/var/lib/drone:/data \ --env=DRONE_GIT_ALWAYS_AUTH=false \ --env=DRONE_GITLAB_SERVER=http://gitlab.xxx.com \ --env=DRONE_GITLAB_CLIENT_ID=9d71bdccd11156913a475891ab082f61677aae816e40911896cb82c17fcfc87e \ --env=DRONE_GITLAB_CLIENT_SECRET=8039aff4d23560355e76e7855b409399c119fdb4005c9d02a2291bfe05d5b6da \ --env=DRONE_RUNNER_CAPACITY=2 \ --env=DRONE_SERVER_HOST=drone.test.com \ --env=DRONE_USER_CREATE=username:xxx,admin:true\ --env=DRONE_SERVER_PROTO=http \ --env=DRONE_TLS_AUTOCERT=false \ --env=DRONE_USER_FILTER=xxx \ --env=DRONE_RPC_SECRET=b00553e8d490f13d296fa0f8a3bdd630 \ --publish=80:80 \ --publish=443:443 \ --restart=always \ --detach=true \ --name=drone \ drone/drone:1.</description></item><item><title>helm本地仓库实践</title><link>dingtalk.pub/post/2309330/</link><pubDate>Thu, 08 Aug 2019 07:03:19 +0000</pubDate><guid>dingtalk.pub/post/2309330/</guid><description>介绍 Helm是查找，共享和使用为Kubernetes构建的软件的最佳方式 安装 直接在github获取： https://github.com/helm/helm 在集群中初始化 首先声明rbac vim rbac-config.yaml
apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system kubectl apply -f ``rbac-config.yaml使用阿里云的仓库初始化 helm init --stable-repo-url https://apphub.aliyuncs.com/ --service-account tiller 初始化后，在kube-system里面会创建一个deploy，但是里面的镜像是谷歌的，需要手动改一下 kubectl edit deployments. -n kube-system tiller-deploy #在helm -v3中删除了tiller组件 --- ... spec: automountServiceAccountToken: true containers: - env: - name: TILLER_NAMESPACE value: kube-system - name: TILLER_HISTORY_MAX value: &amp;#34;0&amp;#34; image: imdingtalk/tiller:v2.</description></item><item><title>HSTS域名实践</title><link>dingtalk.pub/post/2251281/</link><pubDate>Fri, 02 Aug 2019 07:22:04 +0000</pubDate><guid>dingtalk.pub/post/2251281/</guid><description>介绍 大家比较熟悉的是https，在传输数据之前，https会验证客户和服务器直接的信任关系，通过握手来确认这种认证，但是当网站传输协议从 HTTP 到 HTTPS 之后，数据传输真的安全了吗？ 由于用户习惯，通常准备访问某个网站时，在浏览器中只会输入一个域名，而不会在域名前面加上 http:// 或者 https://，而是由浏览器自动填充，当前所有浏览器默认填充的都是http://。一般情况网站管理员会采用了 301/302 跳转的方式由 HTTP 跳转到 HTTPS，但是这个过程总使用到 HTTP 因此容易发生劫持，受到第三方的攻击。如图： 配置 在443的server段添加如下配置： add_header Strict-Transport-Security &amp;#34;max-age=63072000; includeSubdomains; preload&amp;#34;; 在80端口的server端添加重定向：
return 301 https://$server_name$request_uri; 重启服务后的效果 启用hsts后，浏览器还是存在首次访问使用HTTP请求的情况，可以在这里 ，把我们的域名添加到浏览器的预加载里面，浏览器遇到该域名的时候即可实现首次访问的时候就使用https的方式请求，进一步提高安全性 问题 在使用通配的证书的时候，开启HSTS，在使用HTTP请求网站的时候，由于HSTS需要验证，在请求的时候，浏览器会直接使用https发起请求，如我们要访问http://dingtalk.pub;浏览器会直接使用https发起访问，即使用https://dingtalk.pub发起访问，本来如果正常访问http://dingtalk.pub，服务器会直接重定向为https://www.dingtalk.pub;这样不会引起证书问题，但是启用HSTS后，会存在我们的通配证书对根域名 dingtalk.pub的不信任问题，这个，还不知道怎么处理，后面再跟进这个问题将直接导致谷歌浏览器不接受这样不安全的访问 关于这个问题的处理 ~~ ~~额，之前没有发现一个证书是可以颁发给多个域名的，同时颁发给dingtalk.pub和*.dingtalk.pub就可以了嘛，蠢了蠢了;后面再测试，甚至可以颁发给不同的根域
#生成证书的时候这样生成 acme.sh --issue --dns dns_ali -d dingtalk.pub -d *.dingtalk.pub -d *.edgon.cn #安装证书 acme.sh --install-cert -d dingtalk.pub -d *.dingtalk.pub --key-file /etc/nginx/ssl/dingtalk.key --fullchain-file /etc/nginx/ssl/dingtalk.cer --reloadcmd &amp;#34;systemctl restart nginx &amp;#34; server { listen 443; ..... ssl_certificate &amp;#34;/etc/nginx/ssl/dingtalk.</description></item><item><title>基于acme的域名通配证书配置和自动续期</title><link>dingtalk.pub/post/2244982/</link><pubDate>Wed, 31 Jul 2019 14:52:04 +0000</pubDate><guid>dingtalk.pub/post/2244982/</guid><description>简介 在我们的环境中要求，所有的请求都使用https的方式，如果购买商用证书，感觉成本很过分。于是测试使用let&amp;rsquo;s encrypt wildcard的方式来实现
工具介绍 http://acme.sh 一个实现ACME客户端协议的纯Unix shell脚本，用于各种证书的生成。 安装和使用 这个在项目的readme已经很完善了，结合自己的使用，来简要说明下 安装很简单, 一个命令:
curl https://get.acme.sh | sh 生成证书 我这里使用的阿里的域名，使用的dnsapi方式验证 首先去获取API的的key和secret，https://ak-console.aliyun.com/#/accesskey 或 https://ram.console.aliyun.com/permissions 给与dns权限 然后选择该有权限的用户，创建新的accessKey 使用刚刚获取的key来配置acme需要的信息 export Ali_Key=&amp;#34;sdfsdfsdfljlbjkljlkjsdfoiwje&amp;#34; export Ali_Secret=&amp;#34;jlsdflanljkljlfdsaklkjflsa&amp;#34; #Ali_Key和Ali_Secret将保存在〜/.acme.sh/account.conf中，并在需要时重复使用。 #颁发证书 acme.sh --issue --dns dns_ali -d *.demo.dingtalk.pub #建议颁发证书的时候包含根域名 acme.sh --issue --dns dns_ali -d dingtalk.pub -d *.dingtalk.pub acme.sh --issue --dns dns_ali -d *.demo.dingtalk.pub -d demo.dingtalk.pub #安装证书 acme.sh --install-cert -d *.demo.dingtalk.pub --key-file /etc/nginx/ssl/demo.dingtalk.pub.key --fullchain-file /etc/nginx/ssl/demofullchain.cer --reloadcmd &amp;#34;systemctl restart nginx &amp;#34; #至此证书就已经颁发，并且key和cer已经被程序放在/etc/nginx/ssl/目录下，并且定期自动更新 #acme不会修改任何配置，ssl的相关配置，需要自己手动修改配置文件 #自动更新的脚本会被自动添加到定时器 [root@iZwz96589vyznjacr6y3ayZ ~]# crontab -l 58 0 * * * &amp;#34;/root/.</description></item><item><title>mongodb用户权限管理</title><link>dingtalk.pub/post/1960106/</link><pubDate>Wed, 31 Jul 2019 12:56:52 +0000</pubDate><guid>dingtalk.pub/post/1960106/</guid><description>小小记录一下 mongo自己有很多的内置角色，权限的管理，可以按照k8s的理解来处理，即RBAC 。
内置角色可以直接使用，默认的角色有合理的权限划分，所以我们只要创建用户并且关联角色就可以了 角色具有namespace属性，在mongo中，不同数据库（类似namespace）的用户，即使是相同的用户，可以授予不同的角色；即用户和他关联的角色只有用户在特定的数据库下生效，并且，认证的时候也得选择特定的数据库来认证（区别于MySQL的，使用一个用户登录，不需要指定用来认证的数据库） 常规操作 #初始化副本集 use admin rs.initiate() #查看状态 rs.status() rs.isMaster() #添加副本集成员 rs.add(&amp;#39;172.16.13.xx:27018&amp;#39;) #删除节点 rs.remove(&amp;#39;172.16.13.xx:27018&amp;#39;) #查看副本集配置 rs.conf() #恢复步骤 首先恢复一个主节点，然后其他节点，再加入即可 #安全管理 https://docs.mongodb.com/manual/reference/built-in-roles/ #创建用户 use admin db.createUser( {user: &amp;#34;root&amp;#34;,pwd: &amp;#34;root&amp;#34;,roles: [ { role: &amp;#34;userAdminAnyDatabase&amp;#34;, db: &amp;#34;admin&amp;#34; } ]}) #超级用户角色 db.createUser( {user: &amp;#34;cluster&amp;#34;,pwd: &amp;#34;cluster&amp;#34;,roles: [ { role: &amp;#34;clusterAdmin&amp;#34;, db: &amp;#34;admin&amp;#34; } ]}) #集群管理员 db.createUser( {user: &amp;#34;test&amp;#34;,pwd: &amp;#34;test&amp;#34;,roles: [ { role: &amp;#34;readWrite&amp;#34;, db: &amp;#34;test&amp;#34; } ]}) #数据库用户角色，一般用户使用 db.createUser( {user: &amp;#34;dbbackup&amp;#34;,pwd: &amp;#34;dbbackup&amp;#34;,roles: [ { role: &amp;#34;backup&amp;#34;, db: &amp;#34;admin&amp;#34; } ]}) #系统内置备份角色 db.</description></item><item><title>nginx支持webp的图片智适应下发</title><link>dingtalk.pub/post/2198278/</link><pubDate>Thu, 25 Jul 2019 06:22:49 +0000</pubDate><guid>dingtalk.pub/post/2198278/</guid><description>背景 在同一个页面加载多张大图的时候，页面的载入速度，在我们的项目中，取决于图片的大小，页面总加载时间3s，图片加载时间将近1.5s。f12 查看分析，主要的耗时在图片加载，要怎么加快图片的加载呢，首先想到的就是图片的压缩 实践  在nginx中添加对图片的处理逻辑首先在HTTP这里添加：
http { include mime.types; default_type application/octet-stream; xxx... map $http_accept $webp_ext { default &amp;quot;&amp;quot;; &amp;quot;~*webp&amp;quot; &amp;quot;.webp&amp;quot;; } xxxx } 这里的意思是，判断HTTP的请求头是否包含webp，谷歌浏览器在请求图片时候的请求头如下如果包含，这里将设置一个变量webp_ext，它的值是**.webp**,这样，我们的请求的浏览器如果支持webp的图片格式，我们在server层的配置将尝试给浏览器返回一个webp格式的图片 在server层添加如下配置  location ~ .*\.(jpg|jpeg|gif|png)$ { add_header Vary Accept; try_files $uri$webp_ext $uri =404; root /usr/local/openresty/nginx/html; 这里，让我们所有请求图片的请求，使用try_files方法，首先尝试返回$uri$webp_ext所指向的图片，即如果原请求是xxx.png，则nginx这里尝试返回xxx.png.webp 。如果浏览器的请求头中没有webp这个关键字，那么这个变量的值就是原始的uri .即直接返回xxx.png 的原始图片，以适应不支持webp的浏览器 前提是，我们的网站目录中需要包含对应图片的webp格式，这个可以一次性生成，也可以配置nginx-lua在访问的时候实时生成。实时生成后面再研究，我这里在测试环境中，仅尝试一次性生成
yum install libwebp-tools cwebp -q 100 1.png -o 1.png.webp 下图为两个格式的加载时间对比 使用原始图像 使用压缩的webp图像</description></item><item><title>mongodb副本集搭建</title><link>dingtalk.pub/post/1960034/</link><pubDate>Sun, 23 Jun 2019 06:44:57 +0000</pubDate><guid>dingtalk.pub/post/1960034/</guid><description>介绍 副本集是mongoDB副本所组成的一个集群，集群中没有特定的主库，主库是选举产生，如果主库down了，会再选举出一台主库
正常状态下拓扑故障切换过程
副本集有以下特点：1. 最小构成是：primary，secondary，arbiter，一般部署是：primary，2 secondary。2. 成员数应该为奇数，如果为偶数的情况下添加arbiter，arbiter不保存数据，只投票。3. 最大50 members，但是只能有 7 voting members，其他是non-voting members。 部署 下载mongod：https://www.mongodb.com/download-center/community加入到系统路径：chmod +x mongod &amp;amp;&amp;amp; cp mongod /usr/bin/创建用户并设置密码： useradd mongod &amp;amp;&amp;amp; passwd mongod编辑配置文件 # mongod.conf # for documentation of all options, see: # http://docs.mongodb.org/manual/reference/configuration-options/ # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod1.log # Where and how to store data. storage: dbPath: /var/lib/mongo1 journal: enabled: true # engine: # mmapv1: # wiredTiger: # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod1.</description></item><item><title>mongodb使用pymongo批量插入数据</title><link>dingtalk.pub/post/1959886/</link><pubDate>Sun, 23 Jun 2019 04:34:45 +0000</pubDate><guid>dingtalk.pub/post/1959886/</guid><description>背景 在测试mongo相关功能的时候，需要批量插入数据，写一个脚本来批量插入 环境准备 安装 pymongo ，直接使用pip3安装，后面使用insert_many,insert_one的时候会报错如下，是因为默认安装的版本不是最新的，旧版本不支持最新的函数，直接安装最新的，pip3 install -U pymongo但是使用系统默认的python3，运行，会遇到各种问题，因为centos7系统默认的python3的path和pip3安装的东西不匹配，所以这里我直接用/usr/local/python3.6.4/bin,下的执行文件，而不是系统默认的/usr/bin/python3
TypeError: &amp;#39;Collection&amp;#39; object is not callable. If you meant to call the &amp;#39;insert_one&amp;#39; method on a &amp;#39;Collection&amp;#39; object it is failing because no such method exists. 插入数据 import pymongo from pymongo import MongoClient #处理连接信息。建立连接 auth_str=&amp;#39;cluster:cluster@&amp;#39; host_info=&amp;#39;172.16.13.44:27017,172.16.13.44:27018,172.16.13.44:27019&amp;#39; MONGODB=&amp;#39;test&amp;#39; param_str=&amp;#39;?authMechanism=SCRAM-SHA-1&amp;amp;authSource=admin&amp;amp;replicaSet=rs0&amp;#39; uri=&amp;#39;mongodb://%s%s/%s%s&amp;#39; % (auth_str, host_info, MONGODB, param_str) #print(uri) client = MongoClient(uri) #选项数据库插入数据 db=client.test db.test.count() list(db.test.find()) result = db.test.insert_many([{&amp;#39;x&amp;#39;: i} for i in range(1000000)]) result.inserted_ids db.</description></item><item><title>处理api中关于operators.coreos.com的报错</title><link>dingtalk.pub/post/1940892/</link><pubDate>Wed, 19 Jun 2019 10:02:15 +0000</pubDate><guid>dingtalk.pub/post/1940892/</guid><description>背景  在测试环境中的监控一直报错，kubeapierrorhigh,经查看，是default，ns中存在异常的api，通过kubectl api-resources 发现该进程，已无法提供访问处理 删除对应的出问题的api
kubectl delete -n kube-system delete apiservice v1.packages.operators.coreos.com 删除对应的 api后，对应的告警没有了，至于具体这个出错的原因，始终没弄清楚，但是这个api不影响集群，所以先删除了</description></item><item><title>使用kubeadm创建一个高可用的集群</title><link>dingtalk.pub/post/1165522/</link><pubDate>Thu, 06 Jun 2019 16:31:24 +0000</pubDate><guid>dingtalk.pub/post/1165522/</guid><description>准备环境 需要事先准备好haproxy环境和keepalived环境以及容器运行时（参考这里），同时满足以下最小配置的需求至少2G的内存和至少2核的CPU，关闭swap，kubelet必须在swap关闭的情况下才能启动关闭指令：
swapoff -a 开始部署 k8s组件准备 首先安装kubeadm kubelet kubectl ，我们这里使用阿里云的源来安装，所有的节点都需要这个：
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install -y kubelet kubeadm kubectl systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet 镜像准备 kubeadm的安装方式，把k8s的主要组件，kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,coredns,etcd,pause这些组建的都通过镜像使用静态pod的方式启动。首先我们这里要准备好镜像，以便在初始化的时候，加快部署方式，同时排坑。如果你的环境可以访问谷歌那么可以直接通过命令下载：
# 如果你的环境可以访问Google，那么可以通过以下命令下载好所有的镜像 kubeadm config images pull # 如果以上命令执行失败，那么可以使用我准备dockerhub上的源来获取 cat pull_from_docker_hub.sh #!/bin/bash KUBE_VERSION=v1.13.2 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.2.24 COREDNS_VERSION=1.2.6 GCR_URL=k8s.gcr.io HUB=imdingtalk images=(kube-proxy:${KUBE_VERSION} kube-scheduler:${KUBE_VERSION} kube-controller-manager:${KUBE_VERSION} kube-apiserver:${KUBE_VERSION} kube-proxy:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION} etcd:${ETCD_VERSION} coredns:${COREDNS_VERSION} ) for imageName in ${images[@]} ; do docker pull $HUB/$imageName docker tag $HUB/$imageName $GCR_URL/$imageName docker rmi $HUB/$imageName done docker images # 以上，即可直接从docker hub获取镜像，并重新打tag为k8s.</description></item><item><title>mongodb副本集重新配置</title><link>dingtalk.pub/post/1768801/</link><pubDate>Fri, 24 May 2019 07:14:41 +0000</pubDate><guid>dingtalk.pub/post/1768801/</guid><description>背景 最开始部署mongodb副本集的时候使用的是本地地址，导致后面使用连接工具无法远程连接，故需要修改初始化的副本集配置 配置  &amp;gt; db.version(); 4.0.9 &amp;gt; conf=rs.conf() &amp;gt; conf.members[0].host=&amp;#34;172.16.13.x:27017&amp;#34;; &amp;gt; conf.members[1].host=&amp;#34;172.16.13.x:27018&amp;#34;; &amp;gt; conf.members[2].host=&amp;#34;172.16.13.x:27019&amp;#34;; &amp;gt; rs.reconfig(conf,{&amp;#34;force&amp;#34;:true});</description></item><item><title>MYSQL高可用架构之MHA实践</title><link>dingtalk.pub/post/1499735/</link><pubDate>Thu, 16 May 2019 03:38:43 +0000</pubDate><guid>dingtalk.pub/post/1499735/</guid><description>简介 MHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案 经典架构 说明：1.MHA-manager可以安装在除MYSQL-Master以外的任意服务器，唯一的要求是能和所有的DB服务器进行SSH通信2.在所有的MYSQL服务器上安装MHA-node
工作流程 安装部署 keepalive配置 keep主要提供故障切换时候的VIP漂移 #安装 yum install keepalived -y 配置 global_defs { smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_MASTER } vrrp_instance VI_110 { interface ens33 virtual_router_id 110 nopreempt priority 110 advert_int 1 authentication { auth_type PASS auth_pass keepalived } virtual_ipaddress { 192.168.206.144 } } 根据 priority 字段自动判别主备，所以这个配置，在三台服务器之间应该不一致 数据库主从配置 配合5.6数据库新特性，配置主从变得异常简单，需要开启GTID，需要启用这三个参数，所有数据库添加该配置，先主库后从库，添加配置后，需要重启MySQL服务
vim /usr/local/mysql/my.cnf #GTID gtid_mode = on enforce_gtid_consistency = 1 log_slave_updates = 1 创建主从复制的用户，并在从库配置认主 #创建主从复制用户 grant replication slave on *.</description></item><item><title>kubeadm安装的集群的备份和恢复实践</title><link>dingtalk.pub/post/1697464/</link><pubDate>Thu, 16 May 2019 03:36:18 +0000</pubDate><guid>dingtalk.pub/post/1697464/</guid><description>说明 本文档简述了Kubernetes主节点灾备恢复的相关步骤，供在发生k8s master崩溃时操作 **环境：**kubeadm安装的k8s 1.14.1 实践 查看集群的成员和现存的key
ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ member list ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ get / --prefix --keys-only 备份的实现和恢复 etcd集群数据备份，这里我写了一个脚本实现
#!/bin/bash #脚本需要依赖etcdctl，安装 yum install etcd #获取脚本所存放目录 cd `dirname $0` bash_path=`pwd` #脚本名 me=$(basename $0) # delete dir and keep days delete_dirs=(&amp;#34;/data/backup/kubernetes:7&amp;#34;) backup_dir=/data/backup/kubernetes files_dir=(&amp;#34;/etc/kubernetes&amp;#34; &amp;#34;/var/lib/kubelet&amp;#34;) log_dir=$backup_dir/log shell_log=$log_dir/${USER}_${me}.log ssh_port=&amp;#34;22&amp;#34; ssh_parameters=&amp;#34;-o StrictHostKeyChecking=no -o ConnectTimeout=60&amp;#34; ssh_command=&amp;#34;ssh ${ssh_parameters}-p ${ssh_port}&amp;#34; scp_command=&amp;#34;scp ${ssh_parameters}-P ${ssh_port}&amp;#34; DATE=$(date +%F) TIME=$(date &amp;#39;+%Y%m%d%H&amp;#39;) BACK_SERVER=&amp;#34;127.</description></item><item><title>通过域控下发软件安装过程</title><link>dingtalk.pub/post/1676198/</link><pubDate>Mon, 13 May 2019 01:36:18 +0000</pubDate><guid>dingtalk.pub/post/1676198/</guid><description>背景 出于安全考虑，在域中的服务器需要批量安装某软件。 实施 脚本如下 do Set Of = CreateObject(&amp;#34;Scripting.FileSystemObject&amp;#34;) Set objShell = CreateObject(&amp;#34;Shell.Application&amp;#34;) dim OK,oShell OK=False set bag=getobject(&amp;#34;winmgmts:\\.\root\cimv2&amp;#34;) set pipe=bag.execquery(&amp;#34;select * from win32_process where name=&amp;#39;xxx.exe&amp;#39;&amp;#34;) for each match in pipe OK = True msgbox &amp;#34;xxx.exe正在运行。。。&amp;#34; WScript.Quit Next If not OK Then msgbox &amp;#34;xxx.exe没有运行！&amp;#34; Set objShell = CreateObject(&amp;#34;Wscript.Shell&amp;#34;) WScript.Sleep 1000 msgbox &amp;#34;请手动完成xx客户端的安装，否则每次开机都提示&amp;#34;,0 strCommandLine = &amp;#34;\\chinawyny.com\SysVol\domain.com\Policies\{5A850BAF-1901-4071-AD41-1AF69B9BEAA8}\User\Scripts\Logon\DSMClientSetup.exe&amp;#34; objShell.Run(strCommandLine) set WshShell = CreateObject(&amp;#34;WScript.Shell&amp;#34;) WScript.Sleep 1000 msgbox &amp;#34;要显示的内容&amp;#34;,0 WScript.Quit end if loop 这里首先检测目标主机上是否安装了对应的程序并且已经自启动，如果已经启动，则提示软件已经在运行，如果没有运行则提示需要安装，如果是静默安装包，还可以实现静默安装，不弹出setup的安装步骤。软件的安装文件 路径为，域控服务器上的指定地址。 然后编辑域控的策略，添加该脚本即可 </description></item><item><title>k8s 使用kubeconfig和token访问集群</title><link>dingtalk.pub/post/1144573/</link><pubDate>Tue, 30 Apr 2019 05:58:20 +0000</pubDate><guid>dingtalk.pub/post/1144573/</guid><description>概述 kubectl命令访问集群时，默认情况下在$HOME/.kube目录下寻找名为config的配置文件，配置文件中包含集群API地址、端口号、证书等，kubectl据此建构访问集群的上下文。 场景 实际我们在使用中开发需要特定namespace的权限，并且不能有集群的远程权限，使用API访问集群，运维拥有集群的所有控制权，仅有一个集群，于是使用RBAC做权限控制。 制作kubeconfig，同时支持kubectl远程访问和通过token访问API。 环境准备 curl -s -L -o /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 curl -s -L -o /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x /bin/cfssl* 准备好证书请求文件
cat&amp;gt; user-csr.json &amp;laquo;EOF { &amp;ldquo;CN&amp;rdquo;: &amp;ldquo;USER&amp;rdquo;, &amp;ldquo;hosts&amp;rdquo;: [], &amp;ldquo;key&amp;rdquo;: { &amp;ldquo;algo&amp;rdquo;: &amp;ldquo;rsa&amp;rdquo;, &amp;ldquo;size&amp;rdquo;: 2048 }, &amp;ldquo;names&amp;rdquo;: [ { &amp;ldquo;C&amp;rdquo;: &amp;ldquo;CN&amp;rdquo;, &amp;ldquo;ST&amp;rdquo;: &amp;ldquo;BeiJing&amp;rdquo;, &amp;ldquo;L&amp;rdquo;: &amp;ldquo;BeiJing&amp;rdquo;, &amp;ldquo;O&amp;rdquo;: &amp;ldquo;k8s&amp;rdquo;, &amp;ldquo;OU&amp;rdquo;: &amp;ldquo;System&amp;rdquo; } ] } EOF
&amp;gt; &amp;lt;a name=&amp;quot;63199eed&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; ### 定义集群、用户、上下文 制作kubeconf主要有三步： 1. 为配置文件添加集群信息 1.</description></item><item><title>K8S的污点（Taints ）和容忍（Tolerations）</title><link>dingtalk.pub/post/1583929/</link><pubDate>Sun, 28 Apr 2019 03:45:40 +0000</pubDate><guid>dingtalk.pub/post/1583929/</guid><description>节点亲和性是pod的一个属性，它将它们_吸引_到一组节点（作为首选项或硬性要求），Taints 是与之相反的，它允许节点驱逐或抵制一个pod 污点和容忍一起工作以确保不将pod安排到不适当的节点上。 将一个或多个污点应用于节点; 这标志着节点不应该接受任何不能容忍污点的pod。 容忍应用于容器，并允许（但不要求）容器安排到具有匹配的污点的节点上。 概念 使用kubectl taints为节点添加污点。例如 kubectl taint nodes node1 key=value:NoSchedule 在节点node1上放置污点。污点具有键key，值value和污点效果 NoSchedule。这意味着没有pod将能够调度到node1上，除非与pod具有匹配的容忍度。要删除上面命令添加的污点，您可以运行： kubectl taint nodes node1 key:NoSchedule- 在PodSpec中指定容器的容忍Tolerations。以下两种容忍度都“匹配”上面的kubectl污染线所产生的污点，因此具有任何容忍度的容器将能够安排到node1上：
tolerations: - key: &amp;#34;key&amp;#34; operator: &amp;#34;Equal&amp;#34; value: &amp;#34;value&amp;#34; effect: &amp;#34;NoSchedule&amp;#34; tolerations: - key: &amp;#34;key&amp;#34; operator: &amp;#34;Exists&amp;#34; effect: &amp;#34;NoSchedule&amp;#34; 如果key是相同的并且effect相同，则容忍和污点匹配，并且：  operator是Exists（在这种情况下不应指定key），或  operator是Equal，值相等  如果未指定，则operator默认为Equal。 **两种特殊情况： **
带有operator Exists的空键匹配所有key，value和effect，这意味着它将容忍所有内容。  tolerations: - operator: &amp;#34;Exists&amp;#34; 空effect使用key:key匹配所有effect。  tolerations: - key: &amp;#34;key&amp;#34; operator: &amp;#34;Exists&amp;#34; 上面的例子使用了NoSchedule的effect。或者，您可以使用PreferNoSchedule的效果。这是NoSchedule的“首选”或“软”版本 - 系统将尽量避免放置不能容忍节点上污点的pod，但这不是必需的。第三种效果是NoExecute，稍后描述。 您可以在同一节点上放置多个污点，并在同一个pod上放置多个容差。Kubernetes处理多个污点和容忍的方式就像一个过滤器：从所有节点的污点开始，然后忽略pod具有匹配容忍度的那些;剩下的未被忽视的污点对吊舱有明显的影响。特别是  如果至少有一个未被忽略的污点具有效果NoSchedule，那么Kubernetes将不会将pod安排到该节点上  如果没有未被忽略的污点，效果NoSchedule，但至少有一个未被忽略的污点有效PreferNoSchedule然后Kubernetes将尝试不将pod安排到节点  如果存在至少一个具有effect** ** **NoExecute**的未被忽略的污点，则该pod将从该节点逐出（如果它已经在该节点上运行），并且将不会被调度到该节点上（如果它尚未在该节点上运行）。  例如，假设您的节点有这样的污点： kubectl taint nodes node1 key1=value1:NoSchedule kubectl taint nodes node1 key1=value1:NoExecute kubectl taint nodes node1 key2=value2:NoSchedule pod有两种容忍度： tolerations: - key: &amp;#34;key1&amp;#34; operator: &amp;#34;Equal&amp;#34; value: &amp;#34;value1&amp;#34; effect: &amp;#34;NoSchedule&amp;#34; - key: &amp;#34;key1&amp;#34; operator: &amp;#34;Equal&amp;#34; value: &amp;#34;value1&amp;#34; effect: &amp;#34;NoExecute&amp;#34; 在这种情况下，pod将无法安排到节点上，因为没有容忍匹配第三种污点。但是，如果在添加污点时pod已经在节点上运行，它将能够继续运行，因为第三种污点是容器中不能容忍的三种污染中唯一的一种。（pod可以容忍node1有污点 key1=value1:NoSchedule和key1=value1:NoExecute，但是不容忍key2=value2:NoSchedule，但是key2的effect为NoSchedule，意味着在添加污点的时候如果pod已经在该node运行，则不采取处理。如果没有在该node运行，将不会调度到该节点。如果污点 key2=value2:NoExecute，pod容忍不变，那么即使已经在该node运行，添加了该污点后，pod也会被驱逐） 通常情况下，如果将一个带有NoExecute效果的污点添加到一个节点，那么任何不能容忍污染的pod都会立即被驱逐，任何容忍污染的pod都不会被驱逐。但是，对NoExecute效果的容忍可以指定一个可选的tolerationSeconds字段，该字段指示在添加污点后pod将保持绑定到节点的时间。例如， tolerations: - key: &amp;#34;key1&amp;#34; operator: &amp;#34;Equal&amp;#34; value: &amp;#34;value1&amp;#34; effect: &amp;#34;NoExecute&amp;#34; tolerationSeconds: 3600 意味着如果此pod正在运行并且将匹配的污点添加到节点，则pod将保持绑定到该节点3600秒，然后被逐出。如果在此之前删除了污点，则不会驱逐pod。 另外一个pod会被默认添加一个Tolerations： 以上的Tolerations意味着：如果node的状态处于not_ready状态超过5分钟，那么该pod将不能在node上继续执行，将被驱逐。 附注： kubernetes节点失效后pod的调度过程 1.</description></item><item><title>MySQL基于GTID复制的实践</title><link>dingtalk.pub/post/1585713/</link><pubDate>Wed, 24 Apr 2019 06:58:32 +0000</pubDate><guid>dingtalk.pub/post/1585713/</guid><description>介绍 GTID(Global Transaction ID)是MySQL5.6引入的功能，可以在集群全局范围标识事务，用于取代过去通过binlog文件偏移量定位复制位置的传统方式。借助GTID，在发生主备切换的情况下，MySQL的其它Slave可以自动在新主上找到正确的复制位置，这大大简化了复杂复制拓扑下集群的维护，也减少了人为设置复制位置发生误操作的风险。另外，基于GTID的复制可以忽略已经执行过的事务,减少了数据发生不一致的风险。 配置 enforce_gtid_consistency = ON gtid_mode = ON server_id = 9910 binlog_format = row 同时主备库都开启binlog如果不设置级联从库，从库不要设置log_slave_updates参数。这是最合理的设置。建立复制用户 #新版本中建立用户和授权需要分开执行 create user &amp;#39;hh&amp;#39;@&amp;#39;%&amp;#39; identified by &amp;#39;hh&amp;#39;; grant all on *.* to &amp;#39;hh&amp;#39;@&amp;#39;%&amp;#39;; 导出主库数据
mysqldump -uroot -proot -h127.0.0.1 --single-transaction --master-data=2 -R -E --triggers --all-databases &amp;gt; test.sql 在从库source备份，然后使用MASTER_AUTO_POSITION建立同步 change master to master_host=&amp;#39;xxx&amp;#39;, master_user=&amp;#39;repl&amp;#39;, master_password=&amp;#39;repl&amp;#39;, master_port=3306, MASTER_AUTO_POSITION = 1; **启动slave ** start slave 如何修复复制错误 在基于GTID的复制拓扑中，要想修复Slave的SQL线程错误，过去的SQL_SLAVE_SKIP_COUNTER方式不再适用。需要通过设置gtid_next或gtid_purged完成，当然前提是已经确保主从数据一致，仅仅需要跳过复制错误让复制继续下去。比如下面的场景： 在从库上创建表tb1 mysql&amp;gt; set sql_log_bin=0; Query OK, 0 rows affected (0.</description></item><item><title>基于kubeadm搭建的etcd集群的运维实践</title><link>dingtalk.pub/post/1539024/</link><pubDate>Mon, 15 Apr 2019 12:33:51 +0000</pubDate><guid>dingtalk.pub/post/1539024/</guid><description>背景  kubeadm提供了两种高可用方案，用户搭建k8s的高可用环境，详细对比可参考： Options for Highly Available Topology 堆叠的etcd拓扑 特点：
etcd节点和apiserver以及scheduler，controller-manager共存，需要较小的服务器资源  如果node出现问题，那么master节点和etcd节点都将下线 这是kubeadm中的默认拓扑。使用kubeadm init和kubeadm join时使用 experimental-control-plane 作为master 加入集群时候，在控制平面节点上自动创建本地etcd成员 外部etcd集群 特点：
需要额外的服务器搭建etcd集群，相对于堆叠型拓扑，需要双倍的服务器节点 etcd节点和master分离，不像堆叠型拓扑，master或者etcd节点宕机影响更小 常用命令 #etcd需要在etcd的pod内执行 kubectl exec -it -n kube-system etcd-kube-node1 sh #查看集群成员 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ member list #健康检查 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ --endpoints=https://ip1:2379,https://ip2:2379,\ https://ip3:2379 \ endpoint health #删除成员 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \ --cert=/etc/kubernetes/pki/etcd/peer.crt \ --key=/etc/kubernetes/pki/etcd/peer.key \ member remove [member ID] #添加一个值 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.</description></item><item><title>快速批量删除pod</title><link>dingtalk.pub/post/1530580/</link><pubDate>Sat, 13 Apr 2019 06:26:38 +0000</pubDate><guid>dingtalk.pub/post/1530580/</guid><description>背景  因为宿主机资源关系，产生了较多被驱逐的pod，如果不清除，将会一直显示，即使pod已经被重新调度 如何删除 #删除默认ns下的被驱逐的pod kubectl get pods | grep Evicted | awk &amp;#39;{print $1}&amp;#39; | xargs kubectl delete pod #删除指定空间下被驱逐的pod kubectl get pods -n kube-system | grep Evicted | awk &amp;#39;{print $1}&amp;#39; | xargs kubectl delete pod -n kube-system</description></item><item><title>Valine出现Code 403 访问被api域名白名单拒绝，请检查你的安全域名设置</title><link>dingtalk.pub/post/1483686/</link><pubDate>Tue, 09 Apr 2019 16:00:47 +0000</pubDate><guid>dingtalk.pub/post/1483686/</guid><description>背景 前期把hexo的静态页面部署在GH,Valine所有功能正常，但是在部署到Netlif，在自定义的域名开启https之后，Valine出现报错 Code 403: 访问被api域名白名单拒绝，请检查你的安全域名设置 解决 在leancloud的应用&amp;gt;设置&amp;gt;安全中心&amp;gt;Web安全域名中只添加了新的自定义的域名即可解决该问题，</description></item><item><title>centos7安装shadowsocks客户端全局代理</title><link>dingtalk.pub/post/1480766/</link><pubDate>Sun, 07 Apr 2019 10:25:13 +0000</pubDate><guid>dingtalk.pub/post/1480766/</guid><description>前言  最近使用一个开源的项目，在安装依赖的时候，使用的是composer，使用国内的源的情况下，部分依赖，国内的镜像站没有，于是直接使用官方的源。开始之前，取消所有的设置的非官方源 composer config -g --unset repos.packagist 安装  高级加密算法支持  #下载 libsodium 最新版本 wget https://github.com/jedisct1/libsodium/releases/download/1.0.17/libsodium-1.0.17.tar.gz tar xzvf libsodium-1.0.17.tar.gz cd libsodium #编译并且安装 make -j8 &amp;amp;&amp;amp; make install #添加运行库位置并加载运行库 echo /usr/local/lib &amp;gt; /etc/ld.so.conf.d/usr_local_lib.conf ldconfig ss客户端  安装最新的ss客户端 使用以下命令安装最新的客户端（不要仅仅简单的使用 pip install shadowsocks 来安装，因为不支持一些高级加密算法） pip install --upgrade pip pip install https://github.com/shadowsocks/shadowsocks/archive/master.zip -U 配置  vim /etc/shadowsocks.json ##以下配置根据自己的服务器信息填写 { &amp;#34;server&amp;#34;:&amp;#34;x.x.x.x&amp;#34;, #你的 ss 服务器 ip &amp;#34;server_port&amp;#34;:0, #你的 ss 服务器端口 &amp;#34;local_address&amp;#34;: &amp;#34;127.0.0.1&amp;#34;, #本地ip &amp;#34;local_port&amp;#34;:1080, #本地端口 &amp;#34;password&amp;#34;:&amp;#34;password&amp;#34;, #连接 ss 密码 &amp;#34;timeout&amp;#34;:300, #等待超时 &amp;#34;method&amp;#34;:&amp;#34;aes-256-cfb&amp;#34;, #加密方式 &amp;#34;workers&amp;#34;: 1 #工作线程数 } 启动 nohup sslocal -c /etc/shadowsocks.</description></item><item><title>CircleCI+GitHub+hexo 持续集成</title><link>dingtalk.pub/post/1442397/</link><pubDate>Tue, 02 Apr 2019 03:47:41 +0000</pubDate><guid>dingtalk.pub/post/1442397/</guid><description>背景 前期使用Travis做持续集成，Travis针对开源的项目是没有使用限制的，但是你休想使用一个私有项目区构建你的应用(更新：可以build私有项目，不过有个总限制，用完了，就真的没法了)，搜寻之下，暂时准备用CircleCI替代，补充Travis在私有项目中的个人用途。价格参考 &amp;ndash; 单容器单job每月1000分钟build时间，不区分私有还是开源项目。一个hexo的build，每次大概只耗时1分钟。单次build时长限制60min(Travis是50min) 快速开始 这里，直接使用github账户登录。 然后授权要进行ci的repo. 在授权项目根目录新建CircleCI的文件夹 #注意不要少了 . mkdir .circleci vim .circleci/config.yml 示例配置如下：
version: 2 jobs: build: branches: only: - master docker: - image: circleci/node:10 #免费类型的，只能使用一个容器 steps: - checkout # checkout把gh的项目检出，详细参见：https://circleci.com/docs/2.0/configuration-reference/#checkout - run: command: |sed -i &amp;#34;s/GH_TOKEN/${GH_TOKEN}/g&amp;#34; ./_config.yml git config --global user.name $GH_USER git config --global user.email $GH_EMAIL sudo npm install hexo-generator-searchdb --save sudo npm install hexo-wordcount --save sudo npm install --save hexo-blog-encrypt sudo npm install -g hexo-cli sudo npm install hexo-deployer-git --save sudo npm i -g yuque-hexo sudo npm install sudo yuque-hexo sync sudo hexo clean sudo hexo generate sudo hexo deploy 以上做好之后，在gh这里有新的push的时候，CircleCI便会自动构建项目了，同时部署你的hexo网站 后面争取做到在语雀编辑好文章的时候触发自动build</description></item><item><title>GitHub+Travis CI+hexo构建个人博客</title><link>dingtalk.pub/post/1135305/</link><pubDate>Sun, 31 Mar 2019 08:01:54 +0000</pubDate><guid>dingtalk.pub/post/1135305/</guid><description>参考文档:Hexo 博客终极玩法：云端写作，自动部署使用Hexo+Github+TravisCI搭建自动发布的静态博客系统Hexo遇上Travis-CI：可能是最通俗易懂的自动发布博客图文教程
基本参考以上的文章做了完整的一套，此处不再赘诉，以下说下几个坑的地方：
要使用 https://imdingtalk.github.io/ 这种以自己的用户GitHub用户名做域名（不用这种的话，可以使用自己的域名）的话，必须使用master分支做为展示页面。前面教程基本都是用master分支来做hexo源码存放路径的。搞得我后来把仓库名改成 &amp;lt;username&amp;gt;.github.io 之后，自动构建了半天结果页面还是404 。查看设置页面才知道有这坑，但是我前面已经将源码目录和生成后的页面分别放在了master和gh-pages，Google半天也不知道怎么搜索去切换master和gh-pages，自己摸索了下，抱着试一试的心态把远程仓库全部删除了。重新传了下代码解决。仅供参考 做以下操作前，需要先关闭travis-ci的自动构建，避免push了删除的操作后，触发构建，导致代码混乱# 从远端下载最新的代码,进入代码目录 git clone xxx.git cd xxx # 切换到主分支并备份主分支 git checkout master mkdir ../master \cp -rf * ../master # 切换到gh-pages分支并备份该分支 git checkout gh-pages mkdir ../ghpages \cp -rf * ../ghpages # 目录备份完成，直接删除远程仓库代码 git rm -rf * git commit -m &amp;#34;delete gh-pages&amp;#34; git push origin gh-pages git checkout master git rm -rf * git commit -m &amp;#34;delete master&amp;#34; git push origin master # 以上就删除了远端目录，接下来把备份的gh-pages推到master，备份的master推送到gh-pages # change master to gh-pages cd .</description></item><item><title>k8s 共享内存的实现</title><link>dingtalk.pub/post/1391857/</link><pubDate>Tue, 19 Mar 2019 08:13:28 +0000</pubDate><guid>dingtalk.pub/post/1391857/</guid><description>背景  Docker默认共享内存是64M，并且可以通过docker run --shm-size进行修改，Oracle的镜像在docker运行时候可以通过 &amp;ndash;shm-size 指定共享内存的大小，在k8s中没有这个参数可以使用 解决  在k8s中，没有参数可以直接指定变量，只能通过变通的方式实现，查看k8s卷的说明，emptyDir支持Memory，可以将该emptyDir.medium字段设置&amp;quot;Memory&amp;quot; 告诉Kubernetes安装tmpfs（RAM支持的文件系统）,配合deploy，可以实现在pod使用共享内存超出限制的时候驱逐pod
containers: - volumeMounts: - mountPath: /dev/shm name: dshm volumes: - name: dshm emptyDir: medium: Memory sizeLimit: &amp;#34;300Mi&amp;#34; 测试 部署一个deploy-&amp;gt;生成超出限制的请求-&amp;gt;观察pod的events kubectl apply -f deploy.yaml kubectl get pod -o wide # 查看pod所在节点 kubectl exec -it xxxpod bash # 进入容器生成文件 dd if=/dev/zero of=/dev/sdb bs=1300M count=1 参考: Volumes</description></item><item><title>pod 一直terminating 状态的处理</title><link>dingtalk.pub/post/1290093/</link><pubDate>Mon, 25 Feb 2019 01:24:18 +0000</pubDate><guid>dingtalk.pub/post/1290093/</guid><description>使用kubectl delete pod xxx后查看pod状态一直处于terminating 状态，使用kubectl delete pod --grace-period=0 --force可以强制删除pod，但是k8s不会检测是否pod真的被终止了，可能pod会无限期的在集群中运行</description></item><item><title>vps 端口被墙的解决办法</title><link>dingtalk.pub/post/1231675/</link><pubDate>Mon, 11 Feb 2019 02:21:58 +0000</pubDate><guid>dingtalk.pub/post/1231675/</guid><description>背景  自用的vps突然连接不上外网了
排查 检查vps，ping检测:端口检测： 推荐： http://port.ping.pe/可见，vps对应的端口在国内被封禁，此种办法一般修改一个端口即可 解决 在sspanel后台管理处修改对应用户的端口，重启ss服务端再次检测：所有访问点，端口正常通信。</description></item><item><title>k8s DaemonSet 介绍和使用</title><link>dingtalk.pub/post/1201722/</link><pubDate>Wed, 30 Jan 2019 08:11:56 +0000</pubDate><guid>dingtalk.pub/post/1201722/</guid><description>一个_DaemonSet_确保所有（或部分）节点上运行pod的副本，随着节点添加到群集中，将添加Pod。随着节点从群集中删除，这些Pod将被垃圾回收清理，删除DaemonSet将清除它创建的Pod。
DaemonSet的一些典型用法是：  在每个节点上运行 集群存储后台daemon，例如glusterd，ceph。 在每个节点上运行日志收集守护程序，例如fluentd或logstash。 在每个节点上运行节点监控守护进程，比如 Prometheus Node Exporte 在一个简单的例子中，覆盖所有节点的一个DaemonSet将用于每种类型的守护进程。更复杂的设置可能会为单一类型的守护程序使用多个DaemonSet，但对不同的硬件类型使用不同的flags 和/或 不同的内存和CPU请求。
编写DaemonSet 创建一个守护进程集 在yaml文件中描述daemonSet,一个运行fluentd-elasticsearch Docker的yaml文件如下：
apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 基于YAML文件创建DaemonSet: kubectl create -f daemonset.</description></item><item><title>安装和配置kubectl</title><link>dingtalk.pub/post/1164321/</link><pubDate>Sun, 20 Jan 2019 08:36:12 +0000</pubDate><guid>dingtalk.pub/post/1164321/</guid><description>配置源 配置国内的阿里源CentOS / RHEL / Fedora :
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 Debian / Ubuntu :
apt-get update &amp;amp;&amp;amp; apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update 安装 Debian / Ubuntu :
apt-get install -y kubelet CentOS / RHEL / Fedora :
yum install -y kubelet 或者使用二进制包：
# 获取最新二进制包： curl -LO https://storage.</description></item><item><title>解决使用Travis过程中报Permission denied的问题</title><link>dingtalk.pub/post/1164280/</link><pubDate>Sun, 20 Jan 2019 03:15:31 +0000</pubDate><guid>dingtalk.pub/post/1164280/</guid><description>问题描述 在尝试使用Travis来同步谷歌的仓库过程中，使用自己写的脚本，想要添加阿里云的k8s源，遇到报错如下:解决  需要在 travis.yml 文件开头加上 sudo: required 并且在需要执行的命令前加上sudo</description></item><item><title>PV的回收策略小探</title><link>dingtalk.pub/post/1147591/</link><pubDate>Thu, 17 Jan 2019 06:23:24 +0000</pubDate><guid>dingtalk.pub/post/1147591/</guid><description>介绍 回收策略一般在PV或者storageclass中申明，主要有三种策略：Delete Retain Recycle 实践 一个典型的PV申明如下
apiVersion: v1 kind: PersistentVolume metadata: name: nfspv spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain nfs: path: /data/nfs server: 127.0.0.1 此处挂载的是本地的nfs服务器，为了配合测试建立相应的PVC如下：
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfspvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Giya 执行以上两个yaml：
kubectl apply -f nfs-pv.yaml kubectl apply -f nfs-pvc.yaml # 查看pvc或者pvc可以看到他们已经绑定 [root@node1 test]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfspv 2Gi RWO Retain Bound default/nfspvc 11s # 可以看到名字为nfspvc的pvc申明需要的空间为1Gi，k8s自动匹配最相近的PV，所以匹配了名字 # 为nfspv的PV,提供了2Gi存储空间，满足了PVC的要求。 使用一个POD来使用这PVC：</description></item><item><title>无标题</title><link>dingtalk.pub/post/1530018/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>dingtalk.pub/post/1530018/</guid><description/></item><item><title>无标题</title><link>dingtalk.pub/post/2782554/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>dingtalk.pub/post/2782554/</guid><description/></item></channel></rss>