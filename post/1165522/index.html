<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>使用kubeadm创建一个高可用的集群 | snoopy</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="准备环境 需要事先准备好haproxy环境和keepalived环境以及容器运行时（参考这里），同时满足以下最小配置的需求至少2G的内存和至少2核的CPU，关闭swap，kubelet必须在swap关闭的情况下才能启动关闭指令：
swapoff -a 开始部署 k8s组件准备 首先安装kubeadm kubelet kubectl ，我们这里使用阿里云的源来安装，所有的节点都需要这个：
cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet 镜像准备 kubeadm的安装方式，把k8s的主要组件，kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,coredns,etcd,pause这些组建的都通过镜像使用静态pod的方式启动。首先我们这里要准备好镜像，以便在初始化的时候，加快部署方式，同时排坑。如果你的环境可以访问谷歌那么可以直接通过命令下载：
# 如果你的环境可以访问Google，那么可以通过以下命令下载好所有的镜像 kubeadm config images pull # 如果以上命令执行失败，那么可以使用我准备dockerhub上的源来获取 cat pull_from_docker_hub.sh #!/bin/bash KUBE_VERSION=v1.13.2 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.2.24 COREDNS_VERSION=1.2.6 GCR_URL=k8s.gcr.io HUB=imdingtalk images=(kube-proxy:${KUBE_VERSION} kube-scheduler:${KUBE_VERSION} kube-controller-manager:${KUBE_VERSION} kube-apiserver:${KUBE_VERSION} kube-proxy:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION} etcd:${ETCD_VERSION} coredns:${COREDNS_VERSION} ) for imageName in ${images[@]} ; do docker pull $HUB/$imageName docker tag $HUB/$imageName $GCR_URL/$imageName docker rmi $HUB/$imageName done docker images # 以上，即可直接从docker hub获取镜像，并重新打tag为k8s."><meta name=generator content="Hugo 0.83.1"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/dingtalk.pub/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css><meta property="og:title" content="使用kubeadm创建一个高可用的集群"><meta property="og:description" content="准备环境 需要事先准备好haproxy环境和keepalived环境以及容器运行时（参考这里），同时满足以下最小配置的需求至少2G的内存和至少2核的CPU，关闭swap，kubelet必须在swap关闭的情况下才能启动关闭指令：
swapoff -a 开始部署 k8s组件准备 首先安装kubeadm kubelet kubectl ，我们这里使用阿里云的源来安装，所有的节点都需要这个：
cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet 镜像准备 kubeadm的安装方式，把k8s的主要组件，kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,coredns,etcd,pause这些组建的都通过镜像使用静态pod的方式启动。首先我们这里要准备好镜像，以便在初始化的时候，加快部署方式，同时排坑。如果你的环境可以访问谷歌那么可以直接通过命令下载：
# 如果你的环境可以访问Google，那么可以通过以下命令下载好所有的镜像 kubeadm config images pull # 如果以上命令执行失败，那么可以使用我准备dockerhub上的源来获取 cat pull_from_docker_hub.sh #!/bin/bash KUBE_VERSION=v1.13.2 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.2.24 COREDNS_VERSION=1.2.6 GCR_URL=k8s.gcr.io HUB=imdingtalk images=(kube-proxy:${KUBE_VERSION} kube-scheduler:${KUBE_VERSION} kube-controller-manager:${KUBE_VERSION} kube-apiserver:${KUBE_VERSION} kube-proxy:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION} etcd:${ETCD_VERSION} coredns:${COREDNS_VERSION} ) for imageName in ${images[@]} ; do docker pull $HUB/$imageName docker tag $HUB/$imageName $GCR_URL/$imageName docker rmi $HUB/$imageName done docker images # 以上，即可直接从docker hub获取镜像，并重新打tag为k8s."><meta property="og:type" content="article"><meta property="og:url" content="dingtalk.pub/post/1165522/"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-06-06T16:31:24+00:00"><meta property="article:modified_time" content="2019-06-06T16:31:24+00:00"><meta itemprop=name content="使用kubeadm创建一个高可用的集群"><meta itemprop=description content="准备环境 需要事先准备好haproxy环境和keepalived环境以及容器运行时（参考这里），同时满足以下最小配置的需求至少2G的内存和至少2核的CPU，关闭swap，kubelet必须在swap关闭的情况下才能启动关闭指令：
swapoff -a 开始部署 k8s组件准备 首先安装kubeadm kubelet kubectl ，我们这里使用阿里云的源来安装，所有的节点都需要这个：
cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet 镜像准备 kubeadm的安装方式，把k8s的主要组件，kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,coredns,etcd,pause这些组建的都通过镜像使用静态pod的方式启动。首先我们这里要准备好镜像，以便在初始化的时候，加快部署方式，同时排坑。如果你的环境可以访问谷歌那么可以直接通过命令下载：
# 如果你的环境可以访问Google，那么可以通过以下命令下载好所有的镜像 kubeadm config images pull # 如果以上命令执行失败，那么可以使用我准备dockerhub上的源来获取 cat pull_from_docker_hub.sh #!/bin/bash KUBE_VERSION=v1.13.2 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.2.24 COREDNS_VERSION=1.2.6 GCR_URL=k8s.gcr.io HUB=imdingtalk images=(kube-proxy:${KUBE_VERSION} kube-scheduler:${KUBE_VERSION} kube-controller-manager:${KUBE_VERSION} kube-apiserver:${KUBE_VERSION} kube-proxy:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION} etcd:${ETCD_VERSION} coredns:${COREDNS_VERSION} ) for imageName in ${images[@]} ; do docker pull $HUB/$imageName docker tag $HUB/$imageName $GCR_URL/$imageName docker rmi $HUB/$imageName done docker images # 以上，即可直接从docker hub获取镜像，并重新打tag为k8s."><meta itemprop=datePublished content="2019-06-06T16:31:24+00:00"><meta itemprop=dateModified content="2019-06-06T16:31:24+00:00"><meta itemprop=wordCount content="310"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="使用kubeadm创建一个高可用的集群"><meta name=twitter:description content="准备环境 需要事先准备好haproxy环境和keepalived环境以及容器运行时（参考这里），同时满足以下最小配置的需求至少2G的内存和至少2核的CPU，关闭swap，kubelet必须在swap关闭的情况下才能启动关闭指令：
swapoff -a 开始部署 k8s组件准备 首先安装kubeadm kubelet kubectl ，我们这里使用阿里云的源来安装，所有的节点都需要这个：
cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet 镜像准备 kubeadm的安装方式，把k8s的主要组件，kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,coredns,etcd,pause这些组建的都通过镜像使用静态pod的方式启动。首先我们这里要准备好镜像，以便在初始化的时候，加快部署方式，同时排坑。如果你的环境可以访问谷歌那么可以直接通过命令下载：
# 如果你的环境可以访问Google，那么可以通过以下命令下载好所有的镜像 kubeadm config images pull # 如果以上命令执行失败，那么可以使用我准备dockerhub上的源来获取 cat pull_from_docker_hub.sh #!/bin/bash KUBE_VERSION=v1.13.2 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.2.24 COREDNS_VERSION=1.2.6 GCR_URL=k8s.gcr.io HUB=imdingtalk images=(kube-proxy:${KUBE_VERSION} kube-scheduler:${KUBE_VERSION} kube-controller-manager:${KUBE_VERSION} kube-apiserver:${KUBE_VERSION} kube-proxy:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION} etcd:${ETCD_VERSION} coredns:${COREDNS_VERSION} ) for imageName in ${images[@]} ; do docker pull $HUB/$imageName docker tag $HUB/$imageName $GCR_URL/$imageName docker rmi $HUB/$imageName done docker images # 以上，即可直接从docker hub获取镜像，并重新打tag为k8s."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=dingtalk.pub/ class="f3 fw2 hover-white no-underline white-90 dib">snoopy</a><div class="flex-l items-center"></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=dingtalk.pub/dingtalk.pub/post/1165522/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=dingtalk.pub/dingtalk.pub/post/1165522/&text=%e4%bd%bf%e7%94%a8kubeadm%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e9%ab%98%e5%8f%af%e7%94%a8%e7%9a%84%e9%9b%86%e7%be%a4" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=dingtalk.pub/dingtalk.pub/post/1165522/&title=%e4%bd%bf%e7%94%a8kubeadm%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e9%ab%98%e5%8f%af%e7%94%a8%e7%9a%84%e9%9b%86%e7%be%a4" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">使用kubeadm创建一个高可用的集群</h1><time class="f6 mv4 dib tracked" datetime=2019-06-06T16:31:24Z>June 6, 2019</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p></p><h3 id=准备环境>准备环境</h3><p>需要事先准备好haproxy环境和keepalived环境以及容器运行时（参考<a href=https://kubernetes.io/docs/setup/cri/>这里</a>），同时满足以下最小配置的需求  至少2G的内存和至少2核的CPU，关闭swap，kubelet必须在swap关闭的情况下才能启动关闭指令：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>swapoff -a
</code></pre></div><p></p><h3 id=开始部署>开始部署</h3><p></p><h5 id=k8s组件准备>k8s组件准备</h5><p>首先安装kubeadm  kubelet kubectl  ，我们这里使用阿里云的源来安装，所有的节点都需要这个：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat <span style=color:#e6db74>&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style=color:#e6db74>[kubernetes]
</span><span style=color:#e6db74>name=Kubernetes
</span><span style=color:#e6db74>baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
</span><span style=color:#e6db74>enabled=1
</span><span style=color:#e6db74>gpgcheck=1
</span><span style=color:#e6db74>repo_gpgcheck=1
</span><span style=color:#e6db74>gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span><span style=color:#e6db74>EOF</span>
setenforce <span style=color:#ae81ff>0</span>
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet <span style=color:#f92672>&amp;&amp;</span> systemctl start kubelet
</code></pre></div><p></p><h5 id=镜像准备>镜像准备</h5><p>kubeadm的安装方式，把k8s的主要组件，kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,coredns,etcd,pause这些组建的都通过镜像使用静态pod的方式启动。首先我们这里要准备好镜像，以便在初始化的时候，加快部署方式，同时排坑。如果你的环境可以访问谷歌那么可以直接通过命令下载：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># 如果你的环境可以访问Google，那么可以通过以下命令下载好所有的镜像</span>
kubeadm config images pull
<span style=color:#75715e># 如果以上命令执行失败，那么可以使用我准备dockerhub上的源来获取</span>
cat  pull_from_docker_hub.sh
<span style=color:#75715e>#!/bin/bash</span>
KUBE_VERSION<span style=color:#f92672>=</span>v1.13.2
KUBE_PAUSE_VERSION<span style=color:#f92672>=</span>3.1
ETCD_VERSION<span style=color:#f92672>=</span>3.2.24
COREDNS_VERSION<span style=color:#f92672>=</span>1.2.6
GCR_URL<span style=color:#f92672>=</span>k8s.gcr.io
HUB<span style=color:#f92672>=</span>imdingtalk

images<span style=color:#f92672>=(</span>kube-proxy:<span style=color:#e6db74>${</span>KUBE_VERSION<span style=color:#e6db74>}</span>
kube-scheduler:<span style=color:#e6db74>${</span>KUBE_VERSION<span style=color:#e6db74>}</span>
kube-controller-manager:<span style=color:#e6db74>${</span>KUBE_VERSION<span style=color:#e6db74>}</span>
kube-apiserver:<span style=color:#e6db74>${</span>KUBE_VERSION<span style=color:#e6db74>}</span>
kube-proxy:<span style=color:#e6db74>${</span>KUBE_VERSION<span style=color:#e6db74>}</span>
pause:<span style=color:#e6db74>${</span>KUBE_PAUSE_VERSION<span style=color:#e6db74>}</span>
etcd:<span style=color:#e6db74>${</span>ETCD_VERSION<span style=color:#e6db74>}</span>
coredns:<span style=color:#e6db74>${</span>COREDNS_VERSION<span style=color:#e6db74>}</span>
<span style=color:#f92672>)</span>


<span style=color:#66d9ef>for</span> imageName in <span style=color:#e6db74>${</span>images[@]<span style=color:#e6db74>}</span> ; <span style=color:#66d9ef>do</span>
  docker pull $HUB/$imageName
  docker tag  $HUB/$imageName $GCR_URL/$imageName
  docker rmi $HUB/$imageName
<span style=color:#66d9ef>done</span>

docker images
<span style=color:#75715e># 以上，即可直接从docker hub获取镜像，并重新打tag为k8s.gcr.io的前缀，方便使用kubeadm部署</span>
<span style=color:#75715e># kubeadm初始化的时候可以指定不使用官方gcr的仓库的镜像，使用我下面的配置可以省略以上拉镜像的步骤</span>
</code></pre></div><p></p><h5 id=初始化集群>初始化集群</h5><p><strong>在第一个节点</strong>，首先创建一个配置文件 <code>kubeadm-config.yam</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeadm.k8s.io/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterConfiguration</span>
<span style=color:#f92672>kubernetesVersion</span>: <span style=color:#ae81ff>v1.14.1</span>
<span style=color:#f92672>apiServer</span>:
  <span style=color:#f92672>certSANs</span>:
  - <span style=color:#e6db74>&#34;172.16.11.83&#34;</span>
<span style=color:#75715e>#networking:</span>
<span style=color:#75715e>#  podSubnet: 192.168.0.0/16</span>
<span style=color:#f92672>controlPlaneEndpoint</span>: <span style=color:#e6db74>&#34;172.16.11.83:6444&#34;</span>
<span style=color:#f92672>imageRepository</span>: <span style=color:#ae81ff>imdingtalk</span>
---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeproxy.config.k8s.io/v1alpha1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>KubeProxyConfiguration</span>
<span style=color:#f92672>mode</span>: <span style=color:#e6db74>&#34;ipvs&#34;</span>
</code></pre></div><p><strong>kubernetesVersion :</strong>  这个应该为kube的版本，我这里应该改为v1.13.2**controlPlaneEndpoint：**填写负载IP（VIP）或者dns。并加上端口使用ipvs模式，需要加载内核模块</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
<span style=color:#75715e>#配置内核设置</span>
cat <span style=color:#e6db74>&lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
</span><span style=color:#e6db74>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#e6db74>net.bridge.bridge-nf-call-iptables = 1
</span><span style=color:#e6db74>EOF</span>
sysctl --system
sysctl -w net.ipv4.ip_forward<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
setenforce <span style=color:#ae81ff>0</span>
</code></pre></div><p>确保环境是干净的，执行初始化命令</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm init --config<span style=color:#f92672>=</span>kubeadm-config.yaml
</code></pre></div><p>如果顺利的话，你将看到这个输出：<img src=/images/1165522_1.png alt=image.png>不要忘记执行输出中的提示：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown <span style=color:#66d9ef>$(</span>id -u<span style=color:#66d9ef>)</span>:<span style=color:#66d9ef>$(</span>id -g<span style=color:#66d9ef>)</span> $HOME/.kube/config
</code></pre></div><p>记下输出中的这一行，后面新加节点的时候需要用到：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm join 10.10.0.144:6444 --token n90fm9.qsbd52sn4oc8qm4v --discovery-token-ca-cert-hash sha256:cdf01af0bbb5cdb1a7d4795ba357c97ef110430e35ed457c88de568454a8ef38
</code></pre></div><p>以上执行成功后，集群的状态处于一个初始化的状态，需要给节点安装网络插件，才能正常可以用，这里我安装的是Weave 网络插件：</p><pre><code>kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&quot;
# 过一会儿后检查状态
kubectl get pod -n kube-system -w
# 全部为running后执行接下来的事儿
</code></pre><p>把证书文件拷贝到其他节点（<strong>注意：从1.14版本后，不需要接下来的操作，其他的节点基本都是join就完事儿了，不需要拷贝证书</strong>）：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>USER<span style=color:#f92672>=</span>ubuntu <span style=color:#75715e># customizable</span>
CONTROL_PLANE_IPS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.0.0.7 10.0.0.8&#34;</span>
<span style=color:#66d9ef>for</span> host in <span style=color:#e6db74>${</span>CONTROL_PLANE_IPS<span style=color:#e6db74>}</span>; <span style=color:#66d9ef>do</span>
    scp /etc/kubernetes/pki/ca.crt <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:
    scp /etc/kubernetes/pki/ca.key <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:
    scp /etc/kubernetes/pki/sa.key <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:
    scp /etc/kubernetes/pki/sa.pub <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:
    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:
    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:
    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:etcd-ca.key
    scp /etc/kubernetes/admin.conf <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>@$host:
<span style=color:#66d9ef>done</span>
</code></pre></div><p>在其他节点，移动拷贝过来的文件到指定的目录：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>USER<span style=color:#f92672>=</span>ubuntu <span style=color:#75715e># customizable</span>
mkdir -p /etc/kubernetes/pki/etcd
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/sa.pub /etc/kubernetes/pki/
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/sa.key /etc/kubernetes/pki/
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /home/<span style=color:#e6db74>${</span>USER<span style=color:#e6db74>}</span>/admin.conf /etc/kubernetes/admin.conf
</code></pre></div><p>接下来使用刚才记录下来的kubeadm   join 命令来加入到已经存在的节点：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo kubeadm join 192.168.0.200:6443 --token j04n3m.octy8zely83cy2ts --discovery-token-ca-cert-hash sha256:84938d2a22203a8e56a787ec0c6ddad7bc7dbd52ebabc62fd5f4dbea72b14d1f --experimental-control-plane

<span style=color:#75715e># 注意这里的命令，在前面几下来的那个命令的后面加了 --experimental-control-plane 参数，这个参数会让新的节点以master的方式加入集群</span>
<span style=color:#75715e># 查看新的节点的部署情况</span>
kubectl get pod -n kube-system -w 
</code></pre></div><p>如果顺利的话，会是以下输出：可以看到，kubeadm  join  自动的使用证书给etcd的集群添加了member，然后使用静态pod启动了etcd，然后使当前节点，加入了集群作为master<img src=/images/1165522_2.png alt=image.png></p><p>参考文档：<a href=https://kubernetes.io/docs/setup/independent/high-availability/#>Creating Highly Available Clusters with kubeadm</a></p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=dingtalk.pub>&copy; snoopy 2021</a><div></div></div></footer></body></html>